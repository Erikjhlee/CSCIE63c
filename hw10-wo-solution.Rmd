---
title: "CSCI E-63C Week 10 Problem Set"
author: "Erik Lee"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This week we will compare performance of random forest to that of LDA and KNN on a simulated dataset where we know exactly what is the association between predictors and outcome.  The relationship between predictor levels and the outcome will involve interaction that is notoriously difficult to model by methods such as LDA. The following example below illustrates the main ideas on a 3D dataset with two of the three attributes associated with the outcome:

```{r}
# How many observations:
nObs <- 1000
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 1
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction 
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10*nObs*(nClassVars+nNoiseVars)),nrow=10*nObs,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
  deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
  xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
  classTrain <- classTrain * deltaTrain
  deltaTest <- sample(deltaClass*c(-1,1),10*nObs,replace=TRUE)
  xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
  classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
table(classTrain)
# plot resulting attribute levels colored by outcome:
pairs(xyzTrain,col=as.numeric(classTrain))
```

We can see that it is the interaction between the first two variables that has influences the outcome (we simulated it this way, of course!) and that points belonging to each of the two classes cannot be readily separated by a single line in 2D (or a single surface in 3D).

```{r}
# Fit random forest to train data, obtain test error:
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
rfTmpTbl
```

Random forest seems to do reasonably well on such dataset.

```{r}
# Fit LDA model to train data and evaluate error on the test data:
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
ldaTmpTbl
```

LDA, on the other hand, not so good! (not a surprise given what we've seen above).  What about a more flexible method such a KNN?  Let's check it out remembering that k -- number of neighbors -- in KNN is the parameter to modulate its flexibility (i.e. bias-variance tradeoff).

```{r}
# Fit KNN model at several levels of k:
dfTmp <- NULL
for ( kTmp in sort(unique(floor(1.2^(1:33)))) ) {
  knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
  tmpTbl <- table(classTest,knnRes)
  dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
```

We can see from the above that there is a range of $k$ values where test error of KNN is the lowest and it is even lower that that of RF.  Now would be a good moment to think why one would want to choose RF over KNN or vice a versa for modeling the data if the figure above was representative of their true relative performance on a new dataset.

For the purposes of this problem set you can use the code above (probably best to wrap reusable parts of it into function(s)) to generate data with varying numbers of predictors associated with outcome and not, different numbers of observations and differences in the average values of predictors' between two classes as required below. These differences between datasets and parameters of the call to random forest will illustrate some of the factors influencing relative performance of random forest, LDA and KNN classifiers.  When comparing to KNN performance, please choose value(s) of `k` such that it performs sufficiently well -- feel free to refer to the plot above to select useful value(s) of `k` that you would like to evaluate here.  Keep in mind also that the value of `k` cannot be larger than the number of observations in the training dataset.

# Sub-problem 1 (15 points): effect of sample size

Generate training datasets with `nObs=25`, `100` and `500` observations such that two variables are associated with the outcome as parameterized above and three are not associated and average difference between the two classes is the same as above (i.e. in the notation from the above code `nClassVars=2`, `nNoiseVars=3` and `deltaClass=1`).  Obtain random forest, LDA and KNN test error rates on a (for greater stability of the results, much larger, say, with 10K observations) test dataset simulated from the same model.  Describe the differences between different methods and across the sample sizes used here.

```{r, eval=TRUE}
# function to generate classTest data, co-opted from above

# testSizeMag is the magnitude of how much the test data is larger than the training
testDataGen = function(nObs, nClassVars, nNoiseVars, deltaClass=1, classTest=1, testSizeMag=10, xyzTest){
  #xyzTest <- matrix(rnorm(testSizeMag*nObs*(nClassVars+nNoiseVars)),nrow=testSizeMag*nObs,ncol=nClassVars+nNoiseVars)
  for ( iTmp in 1:nClassVars ) {
    deltaTest <- sample(deltaClass*c(-1,1),testSizeMag*nObs,replace=TRUE)
    xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
    classTest <- classTest * deltaTest
  }
  return(list(classTest, xyzTest))
}


# function to generate classTraining data

trainDataGen = function(nObs, nClassVars, nNoiseVars, deltaClass=1, classTrain=1, xyzTrain){
  #xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
  classTrain =1
  for ( iTmp in 1:nClassVars ) {
    deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
    xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
    classTrain <- classTrain * deltaTrain
  
  }
  return(list(classTrain, xyzTrain))
}
```


```{r, eval=TRUE}
# generate training and test data

# set seed for consistent results
set.seed(2)

# observation sizes and predictor counts
nObs = c(25,100,500)
nClassVars = 2
nNoiseVars = 3
deltaClass = 1

# matricies for training and test data
xyzTrain.25 = matrix(rnorm(nObs[1]*(nClassVars+nNoiseVars)), nrow=nObs[1], ncol=nClassVars+nNoiseVars)
xyzTrain.100 = matrix(rnorm(nObs[2]*(nClassVars+nNoiseVars)), nrow=nObs[2], ncol=nClassVars+nNoiseVars)
xyzTrain.500 = matrix(rnorm(nObs[3]*(nClassVars+nNoiseVars)), nrow=nObs[3], ncol=nClassVars+nNoiseVars)

xyzTest.10K = matrix(rnorm(1000*10*(nClassVars+nNoiseVars)), nrow=10*1000, ncol=nClassVars+nNoiseVars) # 10K observations for test

# train data for 25 obs
data.25 = trainDataGen(nObs[1], nClassVars, nNoiseVars, deltaClass, classTrain=1, xyzTrain.25) 
classTrain.25 = unlist(data.25[1])
classTrain.25 = factor(classTrain.25 > 0)
table(classTrain.25)

# train data for 100 obs
data.100 = trainDataGen(nObs[2], nClassVars, nNoiseVars, deltaClass, classTrain=1, xyzTrain.100) # classTrain at index 2
classTrain.100 = unlist(data.100[1])
classTrain.100 = factor(classTrain.100 > 0)
table(classTrain.100)

# train data for 500 obs
data.500 = trainDataGen(nObs[3], nClassVars, nNoiseVars, deltaClass, classTrain=1, xyzTrain.500)
classTrain.500 = unlist(data.500[1])
classTrain.500 = factor(classTrain.500 > 0)
table(classTrain.500)

# test data for all training sizes, 10k test observations
data.10K = testDataGen(1000, nClassVars, nNoiseVars, deltaClass, classTest=1, testSizeMag=10, xyzTest.10K) # 10K observations obs*magnitude = 1000*10 = 10K
classTest.10K = unlist(data.10K[1])
classTest.10K = factor(classTest.10K> 0)
table(classTest.10K)
```

```{r, eval=TRUE}
# Random Forest function

set.seed(3)

randomForestGen = function(xyzTrain, xyzTest, classTrain, classTest){
  rfRes = randomForest(xyzTrain,classTrain)
  rfTmpTbl = table(classTest,predict(rfRes,newdata=xyzTest))
  return(rfTmpTbl)
}

# RF for 25 obs
rf.25 = randomForestGen(xyzTrain.25, xyzTest.10K, classTrain.25, classTest.10K)
rf.25

# RF for 100 obs
rf.100 = randomForestGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
rf.100

# RF for 500 obs
rf.500 = randomForestGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
rf.500

# rf test error
(4088+940)/10000 # 25 obs
(1544+3425)/10000 # 100 obs
(2437+2566)/10000 # 500 obs

```

```{r, eval=TRUE}
set.seed(3)

# LDA
ldaGen = function(xyzTrain, xyzTest, classTrain, classTest){
  ldaRes <- lda(xyzTrain,classTrain)
  ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
  return(ldaTmpTbl)
}

lda.25 = ldaGen(xyzTrain.25, xyzTest.10K, classTrain.25, classTest.10K)
lda.25
lda.100 = ldaGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
lda.100
lda.500 = ldaGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
lda.500

# lda test error
(3420+1622)/10000 # 25 obs
(3684+1328)/10000 # 100 obs
(925+4048)/10000 # 500 obs

```

```{r, eval=TRUE}
set.seed(3)

knnGen = function(nObs, xyzTrain, xyzTest, classTrain, classTest, ldaTmpTbl, rfTmpTbl){
  errors = numeric()
  dfTmp <- NULL
  for ( kTmp in sort(unique(floor(1.2^(1:(log(nObs, base=1.2)))))) ) { # change to 12 to accomodate 25
    knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
    tmpTbl <- table(classTest,knnRes)
    dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
  }
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
}

knn.25 = knnGen(25, xyzTrain.25, xyzTest.10K, classTrain.25, classTest.10K, lda.25, rf.25)
knn.25

knn.100 = knnGen(100, xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K, lda.100, rf.100)
knn.100

knn.500 = knnGen(500, xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K, lda.500, rf.500)
knn.500


```

Above, we see the results for Random Forest, LDA, and KNN for random data with 25, 100, and 500 observations and the corresponding test errors. We see that for Random Forrest, as the number of observations get larger (25, 100, 500) the test error is consistant at 0.5028, 0.4969, 0.5003. On the other hand, LDA's test error decreases slightly as the number of observations increases in the data 0.5042, 0.5012, 0.4973. Based on the graphs of KNN we see that the error rate provides intermediate performance. At 25 observations, KNN has lower test error as K increases. At 100 observations, the test error is LDA and RF regardless of K value. And for 500 observations The test error is higher than RF and LDA for most of the values of K. It is also interesting to see that the level of noise variables has made it harder to get clearer results from confusion matricies and test error.

# Sub-problem 2 (15 points): effect of signal magnitude

For training datasets with `nObs=100` and `500` observations simulate data as shown above with average differences between the two classes that are same as above, half of that and twice that (i.e. `deltaClass=0.5`, `1` and `2`).  Obtain and plot test error rates of random forest, LDA and KNN for each of the six (two samples sizes times three signal magnitudes) combinations of sample size and signal strengths.  As before use large test dataset (e.g. 10K observations or so) for greater stability of the results.  Describe the most pronounced differences across error rates for those datasets: does the increase in the number of observations impact the error rate of the models?  Does change in the magnitude of signal impact their performance?  Are different classifier approaches impacted in a similar way?

```{r, eval=TRUE}
set.seed(3)

# deltaClass = 0.5

# train data for 100 obs
data.100 = trainDataGen(nObs[2], nClassVars, nNoiseVars, deltaClass=0.5, classTrain=1, xyzTrain.100) # classTrain at index 2
classTrain.100 = unlist(data.100[1])
classTrain.100 = factor(classTrain.100 > 0)
#table(classTrain.100)

# train data for 500 obs
data.500 = trainDataGen(nObs[3], nClassVars, nNoiseVars, deltaClass=0.5, classTrain=1, xyzTrain.500)
classTrain.500 = unlist(data.500[1])
classTrain.500 = factor(classTrain.500 > 0)
#table(classTrain.500)

# RF 
rf.100 = randomForestGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
#rf.100
rf.500 = randomForestGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
#rf.500

# lda
lda.100 = ldaGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
#lda.100
lda.500 = ldaGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
#lda.500

# knn
knn.100 = knnGen(100, xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K, lda.100, rf.100)
knn.100
knn.500 = knnGen(500, xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K, lda.500, rf.500)
knn.500



# deltaClass = 1

# train data for 100 obs
data.100 = trainDataGen(nObs[2], nClassVars, nNoiseVars, deltaClass=1, classTrain=1, xyzTrain.100) # classTrain at index 2
classTrain.100 = unlist(data.100[1])
classTrain.100 = factor(classTrain.100 > 0)
#table(classTrain.100)

# train data for 500 obs
data.500 = trainDataGen(nObs[3], nClassVars, nNoiseVars, deltaClass=1, classTrain=1, xyzTrain.500)
classTrain.500 = unlist(data.500[1])
classTrain.500 = factor(classTrain.500 > 0)
#table(classTrain.500)

# RF 
rf.100 = randomForestGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
#rf.100
rf.500 = randomForestGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
#rf.500

# lda
lda.100 = ldaGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
#lda.100
lda.500 = ldaGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
#lda.500

# knn
knn.100 = knnGen(100, xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K, lda.100, rf.100)
knn.100
knn.500 = knnGen(500, xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K, lda.500, rf.500)
knn.500

# deltaClass = 2

# train data for 100 obs
data.100 = trainDataGen(nObs[2], nClassVars, nNoiseVars, deltaClass=2, classTrain=1, xyzTrain.100) # classTrain at index 2
classTrain.100 = unlist(data.100[1])
classTrain.100 = factor(classTrain.100 > 0)
#table(classTrain.100)

# train data for 500 obs
data.500 = trainDataGen(nObs[3], nClassVars, nNoiseVars, deltaClass=2, classTrain=1, xyzTrain.500)
classTrain.500 = unlist(data.500[1])
classTrain.500 = factor(classTrain.500 > 0)
#table(classTrain.500)

# RF 
rf.100 = randomForestGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
#rf.100
rf.500 = randomForestGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
#rf.500

# lda
lda.100 = ldaGen(xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K)
#lda.100
lda.500 = ldaGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)
#lda.500

# knn
knn.100 = knnGen(100, xyzTrain.100, xyzTest.10K, classTrain.100, classTest.10K, lda.100, rf.100)
knn.100
knn.500 = knnGen(500, xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K, lda.500, rf.500)
knn.500

```

Above, we have six total plots for each Delta of 0.5, 1, 2 for 100 observations and 500 observations. The first, third, and fifth graph correspond to Delta of 0.5, 1 and 2 for 100 observations respectively. The second, fourth, and sixth graphs correspond to Delta of 0.5, 1, and 2 for 500 observations respectively. Comparing the 100 observation to 500 observation datasets, we see that across each value of delta, the test error for Random Forrest, LDA, and KNN seems to decrease as the number of observations increases. We also see that higher delta leads to lower test error when comparing each of the 100 observation graphs or 500 observation graphs. Lastly, we see that Random Forrest is affected greater by a change in delta, while LDA and KNN remain pretty consistent with their test errors. 


# Sub-problem 3 (15 points): varying counts of predictors

For all possible pairwise combinations of the numbers of variables associated with outcome (`nClassVars=2` and `5`) and those not associated with the outcome (`nNoiseVars=1`, `3` and `10`) -- six pairwise combinations in total -- obtain and present graphically test errors from random forest, LDA and KNN.  Choose signal magnitude (`deltaClass`) and training data sample size so that this simulation yields non-trivial results -- noticeable variability in the error rates across those six pairwise combinations of attribute counts.  Describe the results: what is the impact of the increase of the number of attributes associated with the outcome on the classifier performance?  What about the number of attributes not associated with outcome - does it affect classifier error rate?  Are different classifier methods affected by these simulation parameters in a similar way?

```{r, eval=TRUE}
# default obs and delta for all combos of classVars and noiseVars
nObs = 500
deltaClass = 2

# create a function to generate plots for each combo 
varyClassNoiseGen = function(nObs=500, nClassVars, nNoiseVars, deltaClass=2, classTrain=1){
  
  data.500 = trainDataGen(nObs, nClassVars, nNoiseVars, deltaClass, classTrain, xyzTrain.500)
  classTrain.500 = unlist(data.500[1])
  classTrain.500 = factor(classTrain.500 > 0)

  # RF 
  rf.500 = randomForestGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)

  # lda
  lda.500 = ldaGen(xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K)


  # knn
  knn.500 = knnGen(500, xyzTrain.500, xyzTest.10K, classTrain.500, classTest.10K, lda.500, rf.500)
  return(knn.500)
}

# nClass Variables = 2
varyClassNoiseGen(nObs, 2, 1, deltaClass, 1) # noise = 1
varyClassNoiseGen(nObs, 2, 3, deltaClass, 1) # noise = 3
varyClassNoiseGen(nObs, 2, 10, deltaClass, 1) # noise = 10

# nClass Variables = 5
varyClassNoiseGen(nObs, 5, 1, deltaClass, 1) # noise = 1
varyClassNoiseGen(nObs, 5, 3, deltaClass, 1) # noise = 3
varyClassNoiseGen(nObs, 5, 10, deltaClass, 1) # noise = 10

```

Above we have grouped different combinations of nClassVariables and nNoise variables. The default nObs is 500 and deltaClass is 2 for each fun. The first three graphs show nClassVariables = 2 for nNoise at 1, 3, 10 respectively. The last three graphs show nClass Variables = 5 for nNoise at 1, 3, 10 respectively. Comparing each nClassVariable graph with the corresponding graph of same nNoiseVariable (i.e. nClass = 2 + nNoise = 3 graph vs. nClass = 5 + nNoise = 3), we see that the variance of the test error increases for KNN as the number of class variables associated with the outcome increases. This can be seen as the first three graphs have smaller ranges for max and min test error than the last three. LDA test error  does not seem to change drastically between 2 class variables and 5 class variables. But we do see that Random Forest test error does decrease when more Class variables associated with the outcome are added from 2 to 5. This may highlight a particular sensitivity to Random Forrest on the amount of class variables in the data.

When comparing across different noise levels, that the amount of variance in test error increases for KNN as more noise variables not associated with the outcome are added to the dataset. This makes sense as noise will increase the variance of the predictions and cause the test error rate to increase, causing the predictions to be less accurate. We see that test error for RF increases as noise increases and test error for LDA does not change as more noise variables are added. This could indicate a particular sensitivity to noise by the Random Forrest algorithm. 

To summarize, we see from comparing different class vairable levels and noise levels that Random Forrests test error is sensitive to changes with test error decreasing with more class variables and increasing with more noise variables. On the other hand, LDA does not show any change in a positive or negative direction for test error as the number of class or noise variables change. KNN has a consistent mean test error across all values of K (approximately 0.500) regardless for class and noise variables. However, the variance in test error does increase as the number of noise variables increases. 

# Sub-problem 4: (15 points): effect of `mtry`

Parameter `mtry` in the call to `randomForest` defines the number of predictors randomly chosen to be evaluated for their association with the outcome at each split (please see help page for `randomForest` for more details).  By default for classification problem it is set as a square root of the number of predictors in the dataset.  Here we will evaluate the impact of using different values of `mtry` on the error rate by random forest.

For `nObs=5000`, `deltaClass=2`, `nClassVars=3` and `nNoiseVars=20` generate data using the above approach, run `randomForest` on it with `mtry=2`, `5` and `10` and obtain corresponding test error for these three models.  Describe the impact of using different values of `mtry` on the test error rate by random forest and compare it to that by LDA/KNN. 

```{r, eval=TRUE}
# default values
nObs = 5000
deltaClass = 2
nClassVars = 3
nNoiseVars = 20

xyzTrain.5000 = matrix(rnorm(nObs*(nClassVars+nNoiseVars)), nrow=nObs, ncol=nClassVars+nNoiseVars)
xyzTest.10K = matrix(rnorm(10*1000*(nClassVars+nNoiseVars)), nrow=10000, ncol=nClassVars+nNoiseVars)

# modified KNN, set max to 33
knnGen2 = function(nObs, xyzTrain, xyzTest, classTrain, classTest, ldaTmpTbl, rfTmpTbl, maxK=33){
  errors = numeric()
  dfTmp <- NULL
  for ( kTmp in sort(unique(floor(1.2^(1:maxK)))) ) { 
    knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
    tmpTbl <- table(classTest,knnRes)
    dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
  }
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
}

# function that varies Mtry
varyMtryGen = function(nObs=5000, nClassVars=3, nNoiseVars=20, deltaClass=2, mtry){
  
  data.10K= testDataGen(1000, nClassVars, nNoiseVars, deltaClass, classTest=1, testSizeMag=10, xyzTest.10K)
  classTest.10K = unlist(data.10K[1])
  classTest.10K = factor(classTest.10K > 0)
  
  data.5000 = trainDataGen(nObs, nClassVars, nNoiseVars, deltaClass, classTrain=1, xyzTrain.5000)
  classTrain.5000 = unlist(data.5000[1])
  classTrain.5000 = factor(classTrain.5000 > 0)

  # RF 
  rfRes <- randomForest(xyzTrain.5000,classTrain.5000, mtry=mtry)
  rf.5000 <- table(classTest.10K,predict(rfRes,newdata=xyzTest.10K))

  # lda
  lda.5000 = ldaGen(xyzTrain.5000, xyzTest.10K, classTrain.5000, classTest.10K)


  # knn
  knn.5000 = knnGen2(nObs, xyzTrain.5000, xyzTest.10K, classTrain.5000, classTest.10K, lda.5000, rf.5000, maxK=33)
  return(knn.5000)
}



varyMtryGen(nObs, nClassVars, nNoiseVars, deltaClass, mtry=2)
varyMtryGen(nObs, nClassVars, nNoiseVars, deltaClass, mtry=5)
varyMtryGen(nObs, nClassVars, nNoiseVars, deltaClass, mtry=10)

```

Above we see the graphs for the dataset of 5000 observations for mtry of 2, 5, and 10 respectively. As mtry increases in each graph, we see the Random Forrest test error decrease. The LDA test error does not seem to noticeably change in the positive or negative direction between values of mtray. And KNN seems consistent in variance. But we do see that Random Forrest test error at mtry = 2 higher than LDA. Then the RF test error is about equal to LDA at mtry = 5. And at mtry = 10, the test error of RF is lower than LDA. This decrease in test error with higher values of mtry might be due to overfitting due to the higher probability of selecting a class variable correlated with the outcome at mtry = 10 as oppose to 2 or 5. As a result, the boostrapped decision trees are likely to contain more important class variables and lower test errors overall for Random Forrests. This may indicate that increasing variable selection at splits can help, but could result in some overfitting of the model to the data at hand. 
