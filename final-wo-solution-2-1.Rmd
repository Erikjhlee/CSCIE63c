---
title: 'CSCI E-63C: Final Exam'
author: "Erik Lee"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

For the final exam/project we will develop classification models using several approaches and compare their performance on a new dataset that is a subset of one of the datasets used in machine learning common task framework (CTF) competitions.  A copy of it split into training (`final-data-train.csv`, with outcome `yn` available) and test (`final-data-test.csv`, stripped of the outcome, for prediction purposes only) datasets is available on our course website in Canvas as a zip-archive of all associated files.

Please notice, that at the end of this final exam/project you will be asked, in addition to the Rmarkdown and HTML files, to also make predictions for the observations in the *test* (not training!) dataset and upload them into Canvas as well.  The expected format for the file with predictions for test dataset is two columns of comma-separated values, one row per observation in *test* dataset, first column -- the observation identifier (column `id` in test dataset) and the second column -- your best model predictions for each observation in the *test* dataset as yes/no indicator.  To illustrate expected format the zip archive contains also a couple of examples of test predictions in this format for your reference as well (`predictions-*.csv` files in `predictions-examples` sub-folder in zip-archive).

One more time, to iterate and emphasize, please notice that this time your submission must consist of the following *three* (not just two, Rmd+html, as usual) items:

* Rmarkdown *.Rmd file with all the calculations you want to receive credit for,
* HTML version of the output generated by your *.Rmd file, and
* **predictions** for the **test** dataset in comma-separated values (CSV) format (file name must have *.csv extension for the file to load in Canvas)

The teaching team invites you to load your predictions (just predictions, just for the test dataset according to the file format shown in the sample files in the zip-archive) into Canvas repeatedly as you work on your models and improve them over the course of this week.  At least daily (or more frequently as we see fit) we will download predictions loaded by everyone by that time and compile a leaderboard in html format of all of them sorted by their accuracy as compared to the true values of the outcome for the test dataset (along with their sensitivity, specificity, etc.).  This list will be made available on our course website in Canvas for everyone in this class in order to see how the performance of their models compares across the rest of the models built by other students in the class.  The first version of the leaderboard posted on the course website at the time when final exam is made available starts with predictions made by those few example files provided in the zip-archive (coin flip, majority vote, etc. -- what do you think Charlie Brown is using to make the predictions?).  Those should be pretty easy to improve upon.

It is 100% up to you whether you want to upload your model predictions over the course of this week, how frequently you want to do it and what you want its results to be called in the leaderboard posted for everyone in the class to see.  We will use the name of the 2nd column in the file with predictions (the one containing yes/no values, not the numerical ids of the observations) as the model name listed in the leaderboard.  If you prefer not to use your name, choose something else instead, sufficiently unique so that it is easier for you to spot your result among all others.  Once again, please check out sample files of dull (majority vote, coin flip, etc.) predictions we have made available and consider how they show up in the leaderboard html file already posted on Canvas website.  Once you are done with final you are expected to load predictions from your best model into Canvas -- that is part of your points total as explained below.

Lastly, the size of this dataset can make some of the modeling techniques run slower than what we were typically encountering in this class.  You may find it helpful to do some of the exploration and model tuning on multiple random samples of smaller size as you decide on useful ranges of parameters/modeling choices, and then only perform a final run of fully debugged and working code on the full dataset.  Please see also the afterword below on the computational demands of this problem set.

# Problem 1: univariate and unsupervised analysis (20 points)

Download and read training and test data into R and prepare graphical and numerical summaries of it: e.g. histograms of continuous attributes, contingency tables of categorical variables, scatterplots of continuous attributes with some of the categorical variables indicated by color/symbol shape, etc.  Whatever you find helpful to think about properties of the data you are about to start using for fitting classification models.

As it is often the case for such contests, the atributes in the dataset are blinded in the sense that no information is available about what those are or what their values mean.  The only information available is that the attribute `yn` is the outcome to be modeled and the attribute `id` is the unique numerical identifier for each observation.  Some of the remaining attributes are clearly categorical (those that are character valued) and some rather obviously continuous (those with numerical values with large number of unique values).  For several of them it is less clear whether it is best to treat them as continuous or categorical -- e.g. their values are numerical but there are relatively few unique values with many observations taking the same value, so that they arguably could be treated as continuous or categorical.  Please idenify them, reflect on how you prefer to handle them and describe this in your own words.

Perform principal components analysis of this data (do you need to scale it prior to that? how would you represent multilevel categorical attributes to be used as inputs for PCA?) and plot observations in the space of the first few principal components indicating levels of some of the categorical attributes of your choosing by the color/shape of the symbol.  Perform univariate assessment of associations between outcome we will be modeling and each of the attributes (e.g. t-test or logistic regression for continuous attributes, contingency tables/Fisher exact test/$\chi^2$ test for categorical attributes).  Summarize your observations from these assessments: does it appear that there are predictors associated with the outcome `yn` univariately? Which predictors seem to be more/less relevant?

```{r, eval=TRUE}
# read in training and test data
train.data = read.csv("final-data-train.csv")
test.data = read.csv("final-data-test.csv")

# set the variable names
var.names = c("yn", "id", "cg", "eg", "re", "ms", "fw", "cl", "hw", "ra", "nc", "se", "ag", "wc", "ne", "oc", "dr")
colnames(train.data) = var.names
colnames(test.data) = var.names[-1]

# summarize data
summary(train.data)
 
# separate the categorical and continous variables vectors

ids = train.data$id # ids are separate and unique for each observation, no impact on predicting outcome

continuous = c(3,7,8,9,11,13) # vector of continous training data
continuous.vars = train.data[, continuous]
colnames(continuous.vars) = var.names[continuous]

categorical.vars = train.data[, -continuous]
categorical.vars = categorical.vars[, -c(1,2)] # remove the outcome yn and id
cate = var.names[-continuous]
cate = cate[-c(1,2)]
colnames(categorical.vars) = cate


```

Above we see the summary of the outcome, yn, and predictor variables for the training data for 29422 observations. The test data is similar with half as many observations and the yn outcome omitted. We see that the given data have 1 outcome, 1 identifier ID column, 6 continuous variables, and 9 categorical variables. The outcome yn has two classes yes and no we will be predicting. The categorical variables have from 2 (se and dr) to 13 categories (oc has most categories). We also see a variety of ranges and variances among the continous variables; ag has a small range and variance from 0.1414-0.8185 and id has large range and variance from 41-595175.

For convenience two dataframes were created for categorical variables and continuous variables, both with all training observations and without the outcome column. Also, the outcome yn column will be converted from yes and no to 1 and 0 numerics to easily represent the results.

```{r, eval=TRUE}
# explore the data visually

# convert the outcome no = 0 and yes = 1
outcome = numeric(length(train.data$yn))
outcome[train.data$yn == "yes"] = 1

# scale the categroicals
#matrix.continuous = as.matrix(continuous.vars)
#df.cont = as.data.frame(scale(matrix.continuous))

# categorical
pairs.df = cbind(data.frame(yn=outcome), continuous.vars)
pairs(pairs.df[, 2:dim(pairs.df)[2]], col=as.numeric(pairs.df$yn+1))
cor(pairs.df)

```

Here we see the pair plots of the 6 continuous variables plotted against each other. The red represents the observations with yn of yes (value 1 as numeric) and the black represents the observations with yn of no (value 0 as numeric). We see in some of the plots that a linear or polynomial decision boundary can be applied to separate the yes no values. Cg and ag, when plotted against other variables, have good example plots where the red and black clusters can be separated with a decision boundary. 

There are cases that can be made for fw and nc too, but not all of their plots are clearly defined. The decent "separable" plots for fw are cg vs. fw, nc vs. fw, and ag vs. fw. For nc, cg vs. nc and nc vs. ag could have decent decision boundary separation. But a few of fw and nc's plots are difficult to sort outcome. We also have cases for the remaining continuous variables of cl and hw too where to an extent they can have decision boundaries created, if we look at cg vs. cl and cg vs. hw. But they have outlier values (columns in the plot) which can complicate classification.

We also do a cursory check of correlation between the continuous variables and the outcomes to see if any collinearity occurs between variables, and to get a rough "rank"" of variables with outcome. There are no significant correlations with the outcome yn, but cg (0.484) and ag (0.343) have the highest correlation and fw (-0.00299) has the lowest correlation to yn. The only two continuous variables with moderatly high correlation and possible collinearily are cg and ag (0.656). The next largest is with hw and cg (0.5362) but this value is moderate correlation. If cg shows the highest correlation (0.484) with yn, we could consider during variable selection using it in place of ag and/or hw if we need to need to reduce the number of predictor variables.

```{r, eval=TRUE}
# histogram of each continous variable
colnames(continuous.vars) = c("cg", "fw", "cl", "hw", "nc", "ag")

old.par <- par(mfcol=c(2,3))
for(var in 1:dim(continuous.vars)[2]){
  var.name = colnames(continuous.vars)[var]
  hist(continuous.vars[, var], xlab=var.name, main=paste("Histogram of ", var.name, sep=""))
}
par(old.par)

# length of unique results for fw, ag, cg, cl
# determine if these predictor variables should be continuous or categorical
length(unique(train.data$fw)) # 29415
length(unique(train.data$ag)) # 110
length(unique(train.data$cg)) # 29422
length(unique(train.data$cl)) # 641

#continuous = c(3,7,9,11) # columns 8 and 13 are cl and ag 
#continuous.vars = train.data[, continuous]
#categorical.vars = train.data[, -continuous]
#categorical.vars = categorical.vars[, -c(1,2)] # remove outcome and id
```

We see the distribution for the continuous variable dataframe. As mentioned in the preface of problem 1, some of these continuous variables can be considered categorical since many of the observations share the same value resulting in a few categories for the variable. Looking at the histogram, we have a case for fw, ag, cg, and cl. Using the length() and unique() functions we can see just how many different results we can get for each variable. We see that fw has 29415 and cg has 29422 both with almost all unique values for each observation. We see ag has 110 unique and cl has 641 unique values. 

Moving foward, we will keep ag and cl as continuous variables since they have so many unique values and a cursory look at the data gives the sense that while many values share the same result, there are also individual or small groups of  observations that have their own values. So it is not worth condensing ag or cl's values to few categories while losing the values of the small groups or individual observations with unique values. We will treat these as continuous distributions/variables.  

```{r, eval=TRUE}
# analyze the categorical variables

# wrapper that creates a contincenty table
contingency.tab = function(table){
  df = table
  row.sums = numeric()
  col.sums = numeric()
  for(col in 1:dim(df)[2]){
    s.col = sum(df[, col])
    col.sums = c(col.sums, s.col)
  }
  for(row in 1:dim(df)[1]){
    s.row = sum(df[row, ])
    row.sums = c(row.sums, s.row)
  }
  if(sum(row.sums) == sum(col.sums)){
    df = cbind(df, row.sums)
    
    tot = sum(row.sums)
    col.sums= c(col.sums, tot)
    df = rbind(df, col.sums)
    return(df)
  }
  return(NULL)
}


# contingency tables for categorical variables
contin.list = list()
for(var in 1:dim(categorical.vars)[2]){
  tab.name = paste("Table for ", colnames(categorical.vars)[var], sep="") 
  show(tab.name)
  tab = table(outcome, categorical.vars[, var]) 
  con.tab = contingency.tab(tab)
  contin.list[[var]] = con.tab # list stores the results of contingency table
  show(con.tab)
}


```

Here we see the contingency table for each of the categorical variables. Since there are a limited number of yes (value of 1) responses for 1

```{r, eval=TRUE}
library(ggplot2)
library(ggfortify)

# perform PCA not scaled to start
pr.no.scale = prcomp(continuous.vars, scale=FALSE)
pr.out = prcomp(continuous.vars, scale=TRUE)


# biplots for first few principle components
# not scaled
autoplot(pr.no.scale, data=continuous.vars, colour=outcome+3, loadings=TRUE, loadings.colour='blue', loadings.label=TRUE, loadings.label.size=5)
#scaled
autoplot(pr.out, data=continuous.vars, colour=outcome+3, loadings=TRUE, loadings.colour='blue', loadings.label=TRUE, loadings.label.size=5)

# scree plot
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Prop. Var. Explained", ylim=c(0,1), type='b', main="Variance explained by each Principal Component")

# mca on categorical.vars
#install.packages(c("FactoMineR", "factoextra") # if not on system
library(FactoMineR) # to pefrom MCA
library(factoextra) # to visualize MCA

mca.out = MCA(categorical.vars, ncp=5, graph=FALSE)

# scree plot for MCA
fviz_screeplot(mca.out, addlabels=TRUE)

# correlation of variables and first two principal compoenents
fviz_mca_var(mca.out, choice="mca.cor", repel=TRUE, ggtheme=theme_minimal())

# PCA for continuous and categorical predictors

# function converts categoricals to arbitrary numeric vectors
convertToNumeric = function(cate.vect){
  fact.vect = factor(cate.vect)
  num.vect = as.numeric(fact.vect)
  return(num.vect)
}


pca.train.data = train.data[, 1:dim(train.data)[2]] # remove yn and id columns
cate.vars = 1:dim(train.data)[2]
cate.vars = cate.vars[-continuous]
cate.vars = cate.vars[-c(1,2)]

# convert all categorical vars to numeric vectors in the pca.train.data
for(v in cate.vars){
  pca.train.data[, v] = convertToNumeric(pca.train.data[, v])
}

pca.train.data = pca.train.data[, -c(1,2)]

# pca for all variables, continuous and categorical
pr.out2 = prcomp(pca.train.data, scale=TRUE)

# biplots for first few principle components for all vars
autoplot(pr.out2, data=pca.train.data, colour=outcome+4, loadings=TRUE, loadings.colour='blue', loadings.label=TRUE, loadings.label.size=5)

# scree plot for all
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Prop. Var. Explained", ylim=c(0,1), type='b', main="Variance explained by each Principal Component for All Vars")

# add outcome and id to pca.train.data dataframe
pca.train.data = cbind(train.data[, c(1,2)], pca.train.data)
```

For multilevel categorical attributes, we need to convert the categories to arbitrary numerical values from 1 to as many categories exist for the variables. Then we can perform PCA on these numerical categories and interpret the resutls based on our original association. 

Another technique Multiple Correspondence Analysis (MCA) which works similarly to PCA but with categorical variables and peforms similar dimensional reduction. The problem with separating out PCA and MCA on the same dataset is interpretability issues. We are not certain about how the variability results for PCA will relate to the variability results of MCA. But we can look at these methods in terms of our outcome yn and consider the predictors (continuous or categorical) separately relating to yn. 
(Source for MCA: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/)

For comparison, we create a function called convertToNumeric() that converts our categorical factors to arbitrary numerical values and pefrom PCA with the continuous variables. Separately, we employ MCA to see how each individual categorical variable relates to yn.

We have several output graphs above. The first is biplot of PCA performed for only contiuous variables that are not scaled. We see that the data for PC2 is very streaky and the loading vectors are hardly visible to see aside from hw and fw. So we proceed by scaling the data. In order to better visualize the affect of these variables and regularize their association with the outcome, we will scale the data from now on.

For the second graph, the biplot of scaled PCA shows the loading vectors for all continuous variables of the training data. We see PC1 (32.36% of variance) across the horizontal and PC2 (17.43% of variance) across the vertical. The green points/cluster represents outcome yn "no" (value 0) and the blue points/cluster represents outcome yn "yes" (value 1). In this PCA we see that cg has a large vector in positive PC1 and cl has a large vector in positive PC2. Nc and ag have large loading vectors in positive PC1 and postive PC2 directions, and hw has large loading vector in postive PC1 and negative PC2 directions. We also see the green "no" cluster centered around 0 PC1 and PC2 and the blue "yes"" cluster centered in the postive PC1 and 0 PC2. 

In the third graph, we show the scree plot of the principal components and their proportions of explained variance for the continuous data. Together, PC1 and PC2 make up approximately 50% (49.795) of the total variation among continuous variables. PC3, PC4, and PC5 have similar variation percentages as PC2 but are omitted in the biplots.

As mentioned before we tested out MCA for the categorical variables, and constructed a scree and variance biplot for the results. Graph 4 shows the scree plot for MCA on only categorical variables and we see that D1 and D2 (Dimension 1 and Dimension 2) explain about 10% (9.8%) of the categorical variable variation. Graph 5 shows that among the categorical variables, ra has a postitive correlation with D1, and wc and oc have postive correlations with D1 and D2. We will consider ra, wc, and oc in our predictive models as well as the others. 

Next, we peform PCA on all the training data variables (continuous and categorical). We convert all of the categorical variables into numeric dataframe columns a created function called convertToNumeric(). We used prcomp() and produced a biplot of PC1 and PC2 and a scree plot for variation. The biplot shows the "no" outcomes in blue and "yes" outcomes in teal, and PC1 in horizontal axis and Pc2 in vertical axis. For the continuous variables we see cg and ag with large vectors in postive PC1, and hw has a large vector in postive PC1 and postive PC2. For categorical variables wc and re have large loading vectors in the direction of postive PC1. On the scree plot, PC1 and PC2 account approximately 20% of the variation (21.72%), not a lot of the variation. The rest is spread among other dimensions and PC's.

Based on the PCA of all variables, we should look at yn in relation to cg, ag, hw for continuous variables and wc, re for categorical variables when modeling outcomes. Conversely, fw, dr, ne have no loading vectors spanning PC1 or PC2 and should be considered to be removed from the model/data/selected against.

```{r, eval=TRUE}
# univariate analysis of attributes with outcome

# continuous, use t-test
for(cont in continuous){
  show(paste("T-test for", colnames(pca.train.data)[cont], sep=" "))
  show(t.test(outcome, pca.train.data[, cont]))
}

# categorical, use Fisher exact test
for(cate in cate.vars){
  show(paste("Chi-Squared test for", colnames(pca.train.data)[cate], sep=" "))
  xvar = as.numeric(pca.train.data[, cate])
  yn = as.numeric(outcome)
  show(chisq.test(x=xvar, y=yn, simulate.p.value=TRUE))
}

```

Here we have the univariate analysis of each variable (continuous and categorical) and their relation to the outcome yn. For continuous variables we run a simple t-test, where we want an absolute t-value over 1.96 (two-sided t-test with alpha = 0.05) and p-value under 0.05. For categorical values, we run a chi-squared test, wehre we want a p-value of less than 0.05. Our continuous variables all pass the test with absolute t-values much larger than 1.96 and p-values of 2.2e-16, all less than 0.05. We can say that the relation of these variables and the outcome are statistically significant and not due to chance. Our categorical variables have ne (p-value = 0.2144) and dr (p-value = 0.1274) failing the chi-sq test. The rest of the categorical variables have p-values of 0.0004998, less than 0.05.

Based on PCA and univariate testing, it seems that most of the variables are associated with outcome yn to some extent. In PCA of all variables, we also see that ne and dr have no loading vectors in PC1 or PC2. Combine this result with the failiure of the chi-squared test, and we have evidence against includeing categorical variables ne and dr in our predictive models. Going forward, we will remove these variables (as well as id because it is not asscoiated with the outcome) from our training data and build models with the remaining continuous and categorical. We can return to variable selection later to improve our model performances by adding or removing attributes in training data.

```{r, eval=TRUE}
# set colnames for train.data.cleaned
colnames(pca.train.data) = colnames(train.data)
# remove id, ne, and dr from the training data
train.data.cleaned = pca.train.data
colnames(train.data.cleaned) = colnames(pca.train.data)
drop.vars = c("id", "ne", "dr")
train.data.cleaned = pca.train.data[, !(names(pca.train.data) %in% drop.vars)]
train.data.cleaned$yn = outcome
```

We will keep fw (continuous) for now, since it is significantly to yn. But will consider removing this variable next to possibly improve performance. 

# Problem 2: logistic regression (20 points)

Develop logistic regression model of the outcome `yn` as a function of multiple predictors in the model.  Which variables are significantly associated with the outcome?  Test model performance on multiple splits of data into training and test subsets and summarize it in terms of accuracy/error/sensitivity/specificity.

```{r, eval=TRUE}
# helper function to assess accuracy, error, sensitivity, and specificity from lecture 9
# print statements are commented out
assess.prediction = function(truth, predicted){
  predicted = predicted[!is.na(truth)]
  truth = truth[!is.na(truth)]
  truth = truth[!is.na(predicted)]
  predicted = predicted[!is.na(predicted)]
  
  #cat("Total cases that are not NA: ", length(truth), "\n", sep="")
  #cat("Correct predictions (accuracy): ", sum(truth==predicted), "(", signif(sum(truth==predicted)*100/length(truth), 3), "%) \n", sep="")
  
  TP = sum(truth==1 & predicted==1)
  TN = sum(truth==0 & predicted==0)
  FP = sum(truth==0 & predicted==1)
  FN = sum(truth==1 & predicted==0)
  
  P = TP+FN
  N = FP+TN
  
  #cat("TPR (sensitivity)=TP/P: ", signif(100*TP/P,3), "%\n", sep="")
  #cat("TNR (specificity)=TN/N: ", signif(100*TN/N,3), "%\n", sep="")
  #cat("PPV (precision)=TP/(TP+FP): ", signif(100*TP/(TP+FP),3), "%n", sep="")
  #cat("FDR (false discovery)=1-PPV: ", signif(100*FP/(TP+FP),3),"%n", sep="")
  #cat("FPR =FP/N=1-TNR: ", signif(100*FP/N,3), "%n", sep="")
  
  results = numeric(4) # stores values for acuray, error, 
  accuracy = signif(sum(truth==predicted)*100/length(truth), 3)
  error = signif(100*(FP+FN)/(P+N),3)
  sensitivity = signif(100*TP/P,3)
  specificity = signif(100*TN/N,3)
  
  results = c(accuracy, error, sensitivity, specificity)
  return(results)
  
}

```

```{r,eval=TRUE}
# creates a formula using a dataframe, and requires the predictor be in column index 1
createFormula = function(df){
  out.name = colnames(df)[1]
  col.names = character()
  for(col in 2:dim(df)[2]){
    col = colnames(df)[col]
    col.names = c(col.names, col)
  }
  right.eq = paste(col.names, collapse="+")
  eq = paste(out.name, "~", sep="")
  eq = paste(eq, right.eq, sep="")
  eq = as.formula(eq)
  return(eq)
}
```


```{r, eval=TRUE}
set.seed(3)

# training data with all variables
train.data.all = pca.train.data[, -2]
train.data.all$yn = outcome

# create logistic regression model
log.equation = createFormula(train.data.all)
log.out = glm(formula=log.equation, data=train.data.all, family=binomial)
summary(log.out)

log.df = NULL
nIters = 10
train = sample(nrow(train.data.cleaned), nrow(train.data.cleaned)/2)
#folds = sample(1:10, nrow(train.data.cleaned), replace=TRUE)

# remove non-significant variables
non.sig = c(4, 5, 6, 8, 12, 16) # re, ms, fw, hw
train.data.log = train.data.all[, -non.sig]

for(i in 1:nIters){
  train = sample(nrow(train.data.all), nrow(train.data.all)/2)
  train.dat = train.data.log[train, ]
  test.dat = train.data.log[-train, ]
  log.equation = createFormula(train.data.log)
  log.out = glm(formula = log.equation, data=train.dat, family=binomial)
  
  log.probs = predict(log.out, test.dat[, -1], type="response")
  log.pred = rep(0, nrow(test.dat))
  log.pred[log.probs > 0.5] = 1
  
  correct = outcome[-train]
  res = assess.prediction(correct, log.pred)
  
  log.df = rbind(log.df, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(log.df$accuracy), sep="")

# average error of logistic regression x10 runs
paste("average error: ", mean(log.df$error), sep="")

# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(log.df$sensitivity), sep="")

# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(log.df$specificity), sep="")

```

Based on the summary of the logistic regression over all variables in the training data, we see that these variables are significantly related to yn: cg, eg, cl, ra, nc, se, wc, and oc (cg, cl, nc are the continuous variables from that bunch). For 10 runs, we computed the average accuracy to be 89.26% with avg. sensitivity at 64.98% and avg. specificity at 97.14%.   

## Extra points problem: interaction terms (5 extra points)

Assess the impact/significance of pairwise interaction terms for all pairwise combinations of covariates used in the model and report the top ten that most significantly improve model fit.

```{r, eval=TRUE, warning=F, message=F}
library(caret)

form = yn~.^2
mm = model.matrix(form, data=train.data.log)
df.pair = as.data.frame(mm)
df.pair = df.pair[, -1]

df.out = data.frame(yn=as.factor(outcome))
df.pair = cbind(df.out, df.pair)

eq = createFormula(df.pair)
mod_fit <- train(eq, data=df.pair, method="glm", family="binomial")

v = varImp(mod_fit, scale=FALSE)
v


```

Above, we have ranking of the top 20 variables of imporatnce. We can see the pairwise covariates ranked among single predictors. The top 10 pariwiase covariates according to our logistic regression fit are: se:oc, cg:eg, eg:oc, cl:nc, nc:se, eg:nc, cg:oc, eg:ra, ra:nc, and eg:cl. 

# Problem 3: linear discriminant analysis (15 points)

Fit linear discriminant analysis model of the outcome `yn` as a function of the rest of covariates in the dataset.  Feel free to decide whether you want to use all of them or a subset of those.  Test resulting model performance on multiple splits of the data into training and test subsets, summarize it in terms of accuracy/error/sensitivity/specificity and compare them to those obtained for logistic regression.

```{r, eval=TRUE}
set.seed(3)
# lda for the same subset as logistic regression
library(MASS)

lda.df = NULL
nIters = 10
for(i in 1:nIters){
  train = sample(nrow(train.data.all), nrow(train.data.all)/2)
  train.dat = train.data.log[train, ]
  test.dat = train.data.log[-train, ]
  lda.equation = createFormula(train.data.log)
  lda.fit = lda(formula=lda.equation, data=train.dat)
  
  lda.pred = predict(lda.fit, test.dat[, -1])
  lda.class = lda.pred$class
  
  correct = outcome[-train]
  res = assess.prediction(correct, lda.class)
  
  lda.df = rbind(lda.df, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

show("LDA with logistic regression variables")

# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(lda.df$accuracy), sep="")

# average error of logistic regression x10 runs
paste("average error: ", mean(lda.df$error), sep="")

# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(lda.df$sensitivity), sep="")

# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(lda.df$specificity), sep="")

# LDA with all variables

lda.df2 = NULL
nIters = 10
for(i in 1:nIters){
  train = sample(nrow(train.data.all), nrow(train.data.all)/2)
  train.dat = train.data.all[train, ]
  test.dat = train.data.all[-train, ]
  lda.equation = createFormula(train.data.all)
  lda.fit = lda(formula=lda.equation, data=train.dat)
  
  lda.pred = predict(lda.fit, test.dat[, -1])
  lda.class = lda.pred$class
  
  correct = outcome[-train]
  res = assess.prediction(correct, lda.class)
  
  lda.df2 = rbind(lda.df2, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

show("LDA with all variables")

# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(lda.df2$accuracy), sep="")

# average error of logistic regression x10 runs
paste("average error: ", mean(lda.df2$error), sep="")

# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(lda.df2$sensitivity), sep="")

# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(lda.df2$specificity), sep="")

```

We decided to run lda using the same predictors and subset of data as the logistic regression. Compared to logistic regression ran 10 times, lda run 10 times did not perform as well. LDA has a higher error of 11.5% compared to log's 10.74%. We also get lower sensitivity at 59.6% (log 64.98%) and marginally higher specificity 97.94% (log 97.14%). The performance on the same variables are not great from LDA.

The second set of scores for LDA with all the variables performs exactly the same as the LDA with the logistic regression selected variables. It does have a minor imrovement of sensitivity to 59.63% but the improvement is 0.03%. There is barely a difference in LDA performing variable selection so we can select the LDA model with all variables.

## Extra points problem: quadratic discriminant analysis (5 extra points)

In our experience attempting to fit quadratic discriminant analysis model of the categorical outcome `yn` on this data results in a rank deficiency related error. Determine on how to correct this error and report resulting model training and test error/accuracy/etc. and how it compares to LDA and logistic regression above. 

```{r, eval=TRUE}
set.seed(3)

# QDA for training data
qda.df = NULL
nIters = 10
for(i in 1:nIters){
  train = sample(nrow(train.data.all), nrow(train.data.all)/2)
  train.dat = train.data.all[train, ]
  test.dat = train.data.all[-train, ]
  qda.equation = createFormula(train.data.all)
  qda.fit = qda(formula=qda.equation, data=train.dat)
  
  qda.pred = predict(qda.fit, test.dat[, -1])
  qda.class = qda.pred$class
  
  correct = outcome[-train]
  res = assess.prediction(correct, qda.class)
  
  qda.df = rbind(qda.df, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

show("QDA with all variables")

# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(qda.df$accuracy), sep="")

# average error of logistic regression x10 runs
paste("average error: ", mean(qda.df$error), sep="")

# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(qda.df$sensitivity), sep="")

# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(qda.df$specificity), sep="")
```

Here we have the results for QDA on all variables. We have average accuracy of 85.89% and average error of 14.11%, sensitivity at 68.39% and specificity at 91.56%. This model does not perform as well as LDA or Logisitic Regression. We have lower accuracy and higher error for our predictions. However, QDA does have higher sensitivity than LDA or Logistic, but is traded off by a much lower specificity.

To correct the rank deficiency error, the outcome yn column in train.data.all is coverted to a numeric column of 0 and 1, where 0 represents no response and 1 represents yes response. The QDA method peforms fine under this dataset.

# Problem 4: random forest (15 points)

Develop random forest model of outcome `yn`. Present variable importance plots and comment on relative importance of different attributes in the model.  Did attributes showing up as more important in random forest model also appear as significantly associated with the outcome by logistic regression?  Test model performance on multiple splits of data into training and test subsets, compare test and out-of-bag error estimates, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of logistic regression and LDA models above.

```{r, eval=TRUE}
set.seed(3)

library(randomForest)
# random forests on all variables
train.data.all$yn = factor(train.data.all$yn)

rf.data = train.data.all[1:(nrow(train.data.all)/10), ] # split data in 10ths to reduce computer workload

rf.formula = createFormula(rf.data)
rf.fit = randomForest(formula=rf.formula, data=rf.data, mtry=15, importance=TRUE)

# plot variable importance
#importance(rf.fit)
varImpPlot(rf.fit)

```

For logistic regression, these variables were selected as significant wiht p-value less than 0.05: cg, eg, cl, ra, nc, se, wc, ne, and oc. The randome forests algorithm favors all these variables except ne. And we saw earlier in PCA that ne has no loading vector in the direction of PC1 and PC2 and it failed the chi-squared test with high p-value. Overall, most variables are favored by random forests except fw, dr, and ne. We can create a random forests model omitting these values and compare performance to logistic regression and LDA.

```{r, eval=TRUE}
set.seed(3)
# remove fw, dr, ne
rem.var = c(5, 13, 15)
train.data.rf = train.data.all[, -rem.var]

# rf on 10 tries
rf.df = NULL
nIters = 10

for(i in 1:nIters){
  train = sample(nrow(train.data.rf), nrow(train.data.rf)/10)
  train.dat = train.data.rf[train, ]
  test.dat = train.data.rf[-train, ]
  
  rf.formula = createFormula(train.data.rf)
  rf.fit = randomForest(formula=rf.formula, data=train.dat, mtry=12)
  
  bag.pred = predict(rf.fit, newdata=test.dat)
  
  correct = outcome[-train]
  res = assess.prediction(correct, bag.pred)
  
  rf.df = rbind(rf.df, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

show("RF with RF selected variables")

# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(rf.df$accuracy), sep="")

# average error of logistic regression x10 runs
paste("average error: ", mean(rf.df$error), sep="")

# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(rf.df$sensitivity), sep="")

# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(rf.df$specificity), sep="")
```

With the Random Forest model, we get an improvment in accuracy at 90.06%. This beats logistic regression at 89.26% and LDA at 88.5%. But we do see a decrease in specificity and increase in sensitivity. Overall, the prediction error does decrease with Random Forrest, and if the goal is to get the most accurate predictions, then this model has the lead in performance. If the dataset is not linked to anything important like Medical diagnosis or banking, then we can place less emphasis on separating out sensitivity (tracking false positives) and specificity (tracking false negatives). 

# Problem 5: SVM (20 points)

Develop SVM model of categorical outcome `yn` deciding on the choice of kernel, cost, etc. that appear to yield better performance.  Test model performance on multiple splits of data into training and test subsets, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of the rest of the models developed above (logistic regression, LDA, random forest).

```{r, eval=TRUE}
# determine optimal parameters for radial svm
set.seed(3)
library(e1071)

# convert yn back to numeric
train.data.all$yn = outcome
train.data.svm = train.data.all
for(c in 1:ncol(train.data.svm)){
  train.data.svm[, c] = as.numeric(train.data.svm[,c])
}


# testing an iteration
#train.dat = train.data.all[folds==1, ]
#test.dat = train.data.all[folds!=1, ]

#svm.formula = createFormula(train.data.all)
#svm.fit = svm(as.factor(yn)~., data=train.dat, kernel="linear", cost=10, scale=F)

#svm.pred = predict(svm.fit, newdata=test.dat)
#correct = outcome[folds!=1]
#res = assess.prediction(correct, svm.pred)

# linear svm with cost=10, results from hw 11
svm.df = NULL
folds = sample(1:15, nrow(train.data.cleaned), replace=TRUE)

for(f in 1:15){
  train.dat = train.data.all[folds==f, ]
  test.dat = train.data.all[folds!=f, ]
  
  svm.formula = createFormula(train.data.all)
  svm.fit = svm(as.factor(yn)~., data=train.dat, kernel="linear", cost=10, scale=FALSE)
  
  
  svm.pred = predict(svm.fit, newdata=test.dat)
  
  correct = outcome[folds!=f]
  res = assess.prediction(correct, svm.pred)
  
  svm.df = rbind(svm.df, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

show("Linear SVM with all variables")
# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(svm.df$accuracy), sep="")
# average error of logistic regression x10 runs
paste("average error: ", mean(svm.df$error), sep="")
# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(svm.df$sensitivity), sep="")
# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(svm.df$specificity), sep="")
```


```{r, eval=TRUE}
set.seed(3)

# radial svm with cost=10 and gamma=0.05, results from hw 11
svm.df2 = NULL
folds = sample(1:15, nrow(train.data.cleaned), replace=TRUE)

for(f in 1:15){
  train.dat = train.data.all[folds==f, ]
  test.dat = train.data.all[folds!=f, ]
  
  svm.formula = createFormula(train.data.all)
  svm.fit = svm(as.factor(yn)~., data=train.dat, kernel="radial", cost=10, gamma=0.05, scale=FALSE)
  
  
  svm.pred = predict(svm.fit, newdata=test.dat)
  
  correct = outcome[folds!=f]
  res = assess.prediction(correct, svm.pred)
  
  svm.df2 = rbind(svm.df2, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

show("Radial SVM with all variables")
# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(svm.df2$accuracy), sep="")
# average error of logistic regression x10 runs
paste("average error: ", mean(svm.df2$error), sep="")
# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(svm.df2$sensitivity), sep="")
# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(svm.df2$specificity), sep="")
```

Here we have results for Linear SVM and Radial SVM. We used a slightly different approach for subsetting the data and training the SVM models on, since SVM takes a while to train on a large number of observations, it is not practical to train on the entire dataset. Instead, we endded up splitting the training data with all variables into 15 subsets. We ran each subset through Linear or Radial SVM and used the remaining 14 subsets to test the data. 

For Linear SVM we have an accuracy of 88.86% and error of 11.13%. Among the other methods, Linear SVM only managed to beat out LDA in performance, but not logistic regression or random forests. It has higher sensitivity than the other methods, but lower specificity. Radial SVM has an average accuracy of 86.92% and error of 13.08%. This performed the worst out of all models compared to logistic classification, LDA, Random Forest, and Linear SVM. Due to its unique decision boundary that creates a radial shape, the poor performance of Radial SVM may indicate that a linear boundary is more suitable for these datasets.

# Problem 6: predictions for test dataset  (10 points)

## Problem 6a: compare logistic regression, LDA, random forest and SVM model performance (3 points)

Compare performance of the models developed above (logistic regression, LDA, random forest, SVM) in terms of their accuracy, error and sensitivity/specificity.  Comment on differences and similarities between them.

We have finally compiled the results from 4 different modeling methods in terms of accuracy, error, sensitivity and specificity: 

Logistic Regression with all variables
[1] "average accuracy: 89.26"
[1] "average error: 10.74"
[1] "average sensitivity: 64.98"
[1] "average specificity: 97.14"

"LDA with all variables"
[1] "average accuracy: 88.5"
[1] "average error: 11.5"
[1] "average sensitivity: 59.63"
[1] "average specificity: 97.93"

"RF with RF selected variables"
[1] "average accuracy: 89.21"
[1] "average error: 10.79"
[1] "average sensitivity: 67.14"
[1] "average specificity: 96.44"

"Linear SVM with all variables"
[1] "average accuracy: 88.8666666666667"
[1] "average error: 11.1333333333333"
[1] "average sensitivity: 62.22"
[1] "average specificity: 97.5866666666667"

"Radial SVM with all variables"
[1] "average accuracy: 86.92"
[1] "average error: 13.08"
[1] "average sensitivity: 59.5466666666667"
[1] "average specificity: 95.8933333333333"

Extra Credit:

[1] "QDA with all variables"
[1] "average accuracy: 85.89"
[1] "average error: 14.11"
[1] "average sensitivity: 68.39"
[1] "average specificity: 91.56"

[1] "NeuralNet with all variables"
[1] "average accuracy: 69.4111111111111"
[1] "average error: 30.5888888888889"
[1] "average sensitivity: 22.1444444444444"
[1] "average specificity: 84.8777777777778"

In the results above, we see the best prformance from Logistic Regression with all variables and second best perfromance from Random Forrest. Note that these algorithms were trained on different sized subsets of the data. Logistic and LDA were trained on half, RF was trained on 1/10th, and Linear and Radial SVM were trained on 1/15th. Logistic Regression had the highest average accuracy (89.26) and lowest average error (10.74). However, Random Forest does have the highest average sensitivity at 67.14% and LDA has the highest average specificity at 97.93%. 

So there are tradeoffs between each model. If we are looking for prediction accuracy, we might use Logistic Regression or Random Forests. If we want high specificity we can go with LDA. And if we want high sensitivity we can go with Random Forests. However, decisions about sensitivity and specificity levels really depend on the purpose behind the data, and if this data is sensitive to error types (eg. Medical Records, Personal Bank Info, etc.) then we care about increasing Sensitivity and reducing false positives. We also have to mention that Logistic Regression and LDA had much shorter runtime and used half the data. RF took very long to run and uses 1/10th the training data, and both Linear and Radial SVM too very long and used 1/15th the training data.

We also mentioned that Radial SVM performed the poorest. Based on our intuition of Radial SVM, it constructs a decision boundary that wraps around a class/cluster in a radial shape. It can create linear-like boundaries with small Gamma values and tight varying boundaries with high Gamma values. We used Gamma = 0.05 which creates a semi-polynomial decision boundary that wraps the cluster. But because Radial SVM performed poorly, and the other methods like Logistic Regression and LDA create linear boundaries, we may believe that a linear-like decision boundary would be suitable for classifying and predicting this data. 

We also see that QDA, which uses a quadratic decision boundary similar to polynomial, peforms slightly behind the rest of the algorithms (Logistic Regression, LDA, Random Forest, Linear SVM), but on par with Radial SVM and much better than NeuralNet. QDA and NeuralNet support the notion that a linear-like decision boundary would be more suitable for performing predictions of data and fitting a proper model.

## Problem 6b: make predictions for the **test** dataset (3 points)

Decide on the model that performs the best and use it to make predictions for the **test** dataset.  This is the dataset that is provided separately from training data without the outcome `yn` that we are modeling here.  Upload resulting predictions in comma-separated values (CSV) format into the Canvas website.  Please check sample files with test dataset predictions for the expected format of the *.csv file with predictions.


```{r, eval=TRUE}
library(randomForest)
set.seed(3)
# predict with random forest

# remove id from test.data
test.data.rf = test.data[, -1]

# convert factors to numeric
cate = c(2,3,4,8,10,12,13,14,15)
for(v in cate){
  test.data.rf[, v] = convertToNumeric(test.data.rf[, v])
}

train = sample(nrow(train.data.rf), nrow(train.data.rf)/10)
train.dat = train.data.rf[train, ]
test.dat = test.data.rf
rf.formula = createFormula(train.data.rf)
rf.fit = randomForest(formula=rf.formula, data=train.dat, mtry=12)

bag.pred = predict(rf.fit, newdata=test.dat)

# convert the binary results to 0 = no and 1 = yes
yn = character(length(bag.pred))
yn[bag.pred==0] = "no"
yn[bag.pred==1] = "yes"
 
# write the test outut to csv 
# my submission titled PredResEL
df.res = NULL
df.res = cbind(data.frame(id=test.data$id), data.frame(yn.rf.EL=yn))
write.csv(df.res, file="RFPredResEL1.csv", row.names=F, quote=F) 

```

Edit: the column for the predictions was renamed to "yn.rf.EL". The first submission just had "yn" as the column name and another submission used "yn" so it is impossible to discern which output was from this result. This submission should make clear the results of this files submission.

## Problem 6c: get better than coin flip by 10% (4 points)

This is not really a problem *per se* but rather a criterion we will go by assessing quality of your predictions for the test dataset.  You get these four points if your predictions for **test** dataset are better than those obtained from a fair coin flip (already shown in leaderboard and as examples of the file format for predictions upload) by at least 10% on **all** four metrics shown in the leaderboard (accuracy, sensitivity, specificity and precision).  But then predictions by the coin flip should not be very difficult to improve upon.  

# Extra 10 points: neural network model

Experiment with fitting neural network models of categorical outcome `yn` for this data and evaluate their performance on different splits of the data into training and test. Compare model performance to that for the rest of classifiers developed above.


```{r, eval=TRUE}
set.seed(3)
library(neuralnet)

yn.pred = numeric(nrow(train.data.all))
nn.df = NULL
# split the data into 10 folds like SVM for computational ease
k = 15 # runs
folds = sample(1:15, nrow(train.data.cleaned), replace=TRUE)
hidden = 10 # start with 10 nodes

for(f in 1:k){
  train.dat = train.data.all[folds==f, ]
  test.dat = train.data.all[folds!=f, ]
  test.dat = test.dat[, -1]
  
  nn.formula = createFormula(train.data.all)
  nn.fit = neuralnet(nn.formula, data=train.dat, hidden=hidden, linear.output=FALSE, err.fct="sse", threshold=0.5)
  
  
  nn.pred = compute(nn.fit, test.dat)
  yn.pred = rep(0, nrow(train.data.all))
  yn.pred[nn.pred$net.result > 0.5] = 1

  correct = outcome[folds!=f]
  res = assess.prediction(correct, svm.pred)
  
  nn.df = rbind(nn.df, data.frame(accuracy=res[1], error=res[2], sensitivity=res[3], specificity=res[4]))
}

nn.df = nn.df[complete.cases(nn.df),] # not all the neural networks converged, remove runs with no results

show("NeuralNet with all variables")
# average accuracy of logistic regression x10 runs
paste("average accuracy: ", mean(nn.df$accuracy), sep="")
# average error of logistic regression x10 runs
paste("average error: ", mean(nn.df$error), sep="")
# average sensitivity of logistic regression x10 runs
paste("average sensitivity: ", mean(nn.df$sensitivity), sep="")
# average specificity of logistic regression x10 runs
paste("average specificity: ", mean(nn.df$specificity), sep="")
```

Above, we have the results for the training data tested on a neural net model with two hidden layers of 5 and 5 nodes. The average accuracy is at 69.11%, error at 30.58%, sensitivity at 22.14% and specificity at 84.87%. Compared to the previous models, NeuralNet peforms worse in all categories and by a sizeable margin. However, we must note we chose a single-layer with 10 nodes and threshold = 0.5. These were based on the Week 12 homework and may not be the best parameters for this dataset, resulting in poor performance.

# An afterword on the computational demands of the final exam

Because during previous offerings of this course there were always several posts on piazza regarding how long it takes to fit various classifiers to the final exam dataset we have added this note here.

First of all, we most definitely do *not* expect you to *have* to buy capacity from AWS to complete this assignment. You certainly can if you want to, but this course is not about that and this dataset is really not *that* big to require it. Something reasonable/useful can be accomplished for this data with middle of the road hardware. For instance, knitting of the entire official solution for the final exam on 8Gb RAM machine with two i5-7200u cores takes about an hour using single-threaded R/Rstudio and this includes both extra points problems as well as various assessments of the performance of different models as function of data size and so on.

Second, your solution should not take hours and hours to compile. If it does, it could be that it is attempting to do too much, or something is implemented inefficiently, or just plain incorrectly - it is impossible for us to comment on this until we see the code when we grade it. In general, it is often very prudent to "start small" -- fit your model on a random subset of data small enough for the model fitting call to return immediately, check how model performance (both in terms of error and time it takes to compute) scales with the size of the data you are training it on (as you increase it in size, say, two-fold several times), for tuning start with very coarse grid of parameter values and given those results decide what it right for you, etc.

Lastly, making the decision about what is right for the problem at hand, how much is enough, etc. is inherent in this line of work. If you choose to conduct model tuning on a subset of the data - especially if you have some assessment of how the choice of tuning parameter and test error is affected by the size of training dataset - it could be a very wise choice.  If it is more efficient for you to knit each problem separately, by all means feel free to do that - just remember to submit each .Rmd and HTML file that comprises your entire solution. On that note, if you end up using any of the unorthodox setups for your calculations (e.g. AWS, parallel processing, multiple machines, etc. - none of which are essential for solving it correctly) please be sure that when we grade we have every relevant piece of code available - we won't be able to grade your work if we are not clear about how the results were obtained.

In the end, the final exam asks you to assess performance of several classification technologies on a new dataset, decide on which classifier is the best and use it to make predictions for the test data. It is very much up to you how exactly you want to go about it.  There could be many versions of correct and informative solution for that (as there could be just as many if not more that are completely wrong).

As always, best of luck - we are practically done here!
