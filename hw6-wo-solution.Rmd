---
title: 'CSCI E-63C: Week 6 Problem Set'
author: 'Erik Lee'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

In this problem set we will exercise some of the unsupervised learning approaches on [2016 Global Health Observatory data](http://www.who.int/gho/publications/world_health_statistics/2016/en/).  It is available at that website in the form of [Excel file](http://www.who.int/entity/gho/publications/world_health_statistics/2016/whs2016_AnnexB.xls?ua=1), but its cleaned up version ready for import into R for further analyses is available at CSCI E-63C canvas course web site [whs2016_AnnexB-data-wo-NAs.txt](https://canvas.harvard.edu/files/6123649/download?download_frd=1).  The cleaning and reformatting included: merging data from the two parts of Annex B, reducing column headers to one line with short tags, removal of ">", "<" and whitespaces, conversion to numeric format and replacement of undefined values (as indicated by en-dash'es in the Excel) with corresponding averages of those attributes.  The code that was used to format merged data is shown at the end of this document for your reference only.  You are advised to save yourself that trouble and start from preformatted text file available at the course website as shown above.  The explicit mapping of short variable names to their full description as provided in the original file is available in Excel file [whs2016_AnnexB-reformatted.xls](https://canvas.harvard.edu/files/6123599/download?download_frd=1) also available on the course canvas page.  Lastly, you are advised to download a local copy of this text file to your computer and access it there (as opposed to relying on R ability to establish URL connection to canvas that potentially requires login etc.)

Short example of code shown below illustrates reading this data from a local copy on your computer (assuming it has been copied into current working directory of your R session -- `getwd()` and `setwd()` commands are helpful to find out what is it currently and change it to desired location) and displaying summaries and pairs plot of five (out of almost 40) arbitrary chosen variables.  This is done for illustration purposes only -- the problems in this set expect use of all variables in this dataset.

```{r WHS}
whsAnnBdatNum <- read.table("whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whsAnnBdatNum[,c(1,4,7,10,17)])
pairs(whsAnnBdatNum[,c(1,4,7,10,17)])
```

In some way this dataset is somewhat similar to the `USArrests` dataset extensively used in ISLR labs and exercises -- it collects various continuous statistics characterizing human population across different territories.  It is several folds larger though -- instead of `r nrow(USArrests)` US states and `r ncol(USArrests)` attributes in `USArrests`, world health statistics (WHS) data characterizes `r nrow(whsAnnBdatNum)` WHO member states by `r ncol(whsAnnBdatNum)` variables.  Have fun!

The following problems are largely modeled after labs and exercises from Chapter 10 ISLR.  If anything presents a challenge, besides asking questions on piazza (that is always a good idea!), you are also encouraged to review corresponding lab sections in ISLR Chapter 10.

# Problem 1: Principal components analysis (PCA) (25 points)

The goal here is to appreciate the impact of scaling of the input variables on the result of the principal components analysis.  To that end, you will first survey means and variances of the attributes in this dataset (sub-problem 1a) and then obtain and explore results of PCA performed on data as is and after centering and scaling each attribute to zero mean and standard deviation of one (sub-problem 1b).

## Sub-problem 1a: means and variances of WHS attributes (5 points)

Compare means and variances of the *untransformed* attributes in the world health statisics dataset -- plot of variance vs. mean is probably the best given the number of attributes in the dataset.  Function `apply` allows to apply desired function (e.g. `mean` or `var` or `sd`) to each row or column in the table.  Do you see all `r ncol(whsAnnBdatNum)` attributes in the plot, or at least most of them?  (Remember that you can use `plot(inpX,inpY,log="xy")` to use log-scale on both horizontal and vertical axes.)  Is there a dependency between attributes' averages and variances? What is the range of means and variances when calculated on untransformed data?  Which are the top two attributes with the highest mean or variance?  What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

```{r, eval=TRUE}
# vectors for mean and variance of the variables(columns)
whs_matrix = data.matrix(whsAnnBdatNum)
means_x = apply(whs_matrix, MARGIN=2, FUN=mean) # all attribute means
vars_y = apply(whs_matrix, MARGIN=2, FUN=var) # all attribute variances

# plot untransformed of variance vs. mean
plot(means_x, vars_y, xlab="Mean", ylab="Variance", main="Variance vs. Means Plot 1")

summary(means_x)
summary(vars_y)

# remove the first outlier
max_mean1 = max(means_x)
max_var1 = max(vars_y)
means_1 = means_x[means_x < max(means_x)] # rm max of means
vars_1 = vars_y[vars_y < max(vars_y)] # rm max of variances

# remove the scond outlier
max_mean2 = max(means_1)
max_var2 = max(vars_1)
means_2 = means_1[means_1 < max(means_1)] # rm max of means again
vars_2 = vars_1[vars_1 < max(vars_1)] # rm max of variances again

# untransformed plot without the two highest values
plot(means_2, vars_2, xlab="Mean", ylab="Variance", main="Variance vs. Means with Two Outliers Removed")

# find what columns the outliers correspond to
# outlier/leverage 1 from whole attribute set
means_x[means_x == max_mean1]
vars_y[vars_y == max_var1]

# outlier/leverage 2 from attribute set with first outlier removed
means_x[means_x == max_mean2]
vars_y[vars_y == max_var2]

# futher plot
means_3 = means_2[means_2 < 90] # 90 came from testing, excludes 5 outliers
vars_3 = vars_2[vars_2 < 5000] # 5000 also came from testing, excludes 5 outliers
plot(means_3,vars_3, xlab="Mean", ylab="Variance", main="Variance vs. Means with Five Outliers Removed")
```

There is no clear dependency between the means and variances of the attributes in the data set. On the second plot, which has the two highest values for mean and variance removed, there seems to be no association between mean and variance, except for the three points furthest to the right. Plot 3 shows an even closer view with the three far right points removed, and still no dependency found. 

The range for means is from 0 to 9817345 for the non-transformed attributes. The range for the variances is from 0 to 1.958e+15. The first point with highest mean and variance is attribute INTINTDS. The second point with highest mean and variance is TOTPOP. 

If PCA was applied without transforming the data, the result would display arbitrary ranges for the principle components (PCs). We would see heavy influence from outlier/leverage attributes such as INTINTDS and TOTPOP explain a disproporionate amount of variance compared to the other attributes. As a result, no clear associations can be made between the attributes and among the observations, even though they may be important in the context of classifying our data.


## Sub-problem 1b: PCA on untransformed and scaled WHS data (20 points)

Perform the steps outlined below *both* using *untransformed* data and *scaled* attributes in WHS dataset (remember, you can use R function `prcomp` to run PCA and to scale data you can either use as input to `prcomp` the output of `scale` as applied to the WHS data matrix or call `prcomp` with parameter `scale` set to `TRUE`). To make it explicit, the comparisons outlined below have to be performed first on the unstransformed WHS data and then again on scaled WHS data -- you should obtain two sets of results that you could compare and contrast.

1. Obtain results of principal components analysis of the data (by using `prcomp`)

```{r, eval=TRUE}
pca_untransformed = prcomp(whsAnnBdatNum, scale=FALSE)
pca_transformed = prcomp(whsAnnBdatNum, scale=TRUE)

dim(pca_untransformed$x)
```


2. Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`)

```{r,eval=TRUE}
plot(pca_untransformed, xlab="PCA", main="Unscaled/Transfromed PCA for WHS")
plot(pca_transformed, xlab="PCA", main="Scaled PCA for WHS")
```


3. Generate plot of the two first principal components using `biplot`.  Which variables seem to predominantly drive the results of PCA when applied to untransformed data?

```{r,eval=TRUE}
biplot(pca_untransformed)
biplot(pca_transformed)
```


  + Please note that in case of untransformed data you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?
  
  While running the biplot() function, a "zero-length arrow is of indeterminate angle and so skipped" error appears. This appears because the arrows are too small to fit on the plot due to their very small variance compared to the two largest variances. This happens for almost all the variables except TOTPOP and INTINTDS, which are plotted properly.
  
4. The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.

```{r,eval=TRUE}
#pca_untransformed$rotation[,1:2]
#abs(pca_transformed$rotation[,1:2])

whs_pca_nonscale_abs = abs(pca_untransformed$rotation[,1:2])
whs_pca_scale_abs = abs(pca_transformed$rotation[,1:2])

# max for nonscaled WHS PC1
whs_pca_nonscale_abs[,1][whs_pca_nonscale_abs[,1] == max(whs_pca_nonscale_abs[,1])]
# max for nonscaled WHS PC2
whs_pca_nonscale_abs[,2][whs_pca_nonscale_abs[,2] == max(whs_pca_nonscale_abs[,2])]

# max for scaled WHS PC1
whs_pca_scale_abs[,1][whs_pca_scale_abs[,1] == max(whs_pca_scale_abs[,1])]
# max for scaled WHS PC2
whs_pca_scale_abs[,2][whs_pca_scale_abs[,2] == max(whs_pca_scale_abs[,2])]

```

  + What attributes have the largest (by their absolute value) loadings for the first and second principal component?
  
  Consistent with the findings in problem 1a and the biplot above for the untransfromed data, we find the largest absolute attribute value for PC1 is INTINTDS and the largest absolute attribute value for PC2 is TOTPOP. If we look at the biplot for untransformed data, we see that the arrow for INTINTDS has the largest span across PC1 and TOTPOP has the second largest span across PC2. 
  
  + How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?
  
  Compared to the findings in 1a of the means and variances of all the attributes, the findings for PC1 and PC2 are the same as the outliers/leverage points for the largest and second largest means and variances. We found that INTINTDS had the largest mean and variance of all the attributes so it makes sense that PC1 would capture INTINTDS variance. And similarly, TOTPOP had the second largest mean and variance among the attributes and is captured by PC2, which captrues the second largest source of variance. 
  
5. Calculate percentage of variance explained (PVE) by the first five principal components (PCs).  You can find an example of doing this in ISLR Chapter 10.4 (Lab 1 on PCA).

```{r, eval=TRUE}
# first five PVE for nonscaled data
pr_noscale_var = pca_untransformed$sdev^2  
pve_noscale = pr_noscale_var/sum(pr_noscale_var)
pve_noscale[1:5]

# first five PVE fro scaled data
pr_scale_var = pca_transformed$sdev^2
pve_scale = pr_scale_var/sum(pr_scale_var)
pve_scale[1:5]
```


Now that you have PCA results when applied to untransformed and scaled WHS data, please comment on how do they compare and what is the effect of scaling?  What dataset attributes contribute the most (by absolute value) to the top two principal components in each case (untransformed and scaled data)?  What are the signs of those contributions?  How do you interpret that?

Scaling does not bias the Principle Component Analysis of data with large measurement magnitudes. We saw that in the unscaled data, there was a heavy bias for INTINTDS (PC1) and TOTPOP (PC2), which have very large ranges and large variances. Scaling ensures that the data's attributes have consistant variances that are comparable and not dependent on the range by normalizing the mean and standard of deviations. This makes the variances more interpretable and resulted in a max PC1 for LIFEXPB.F and max PC2 for HOMICIDE. 

In the unscaled data, both INTINTDS and TOTPOP have positive signs. For scaled data, LIFEXPB.F has a negative sign and HOMICIDE has a positive sign. The sign is just an indicator of how the attributes relate with each other. Flipping the signs in the principle component does not affect magnitude, but just changes the orientation of the attributes in the PC space. We see that INTINTDS and TOTPOP have positive signs so they are have postive covariance with each other (vary postively with the principle component, nothing is said about correlation or magnitude). And we see that LIFEXPB.F and HOMICIDE with opposite signs have a negative covariance with each other; they diverge in direction when mapped onto the space of a principle component. We see both these cases when we check the covariances of the attributes.

```{r, eval=TRUE}
# summary of two top unscaled attributes
summary(whsAnnBdatNum["INTINTDS"])
summary(whsAnnBdatNum["TOTPOP"])

# signs for the max PC1 and PC2 for nonscaled and scaled data
pca_untransformed$rotation[c("INTINTDS", "TOTPOP"),][,1:2]
pca_transformed$rotation[c("LIFEXPB.F", "HOMICIDE"),][,1:2]

# looking at covariance in relation to the sign of the signs for PCA
cov(whsAnnBdatNum$INTINTDS, whsAnnBdatNum$TOTPOP)
cov(whsAnnBdatNum$LIFEXPB.F, whsAnnBdatNum$HOMICIDE)

```


Please note, that the output of `biplot` with almost 200 text labels on it can be pretty busy and tough to read.  You can achieve better control when plotting PCA results if instead you plot the first two columns of the `x` attribute in the output of `prcomp` -- e.g. `plot(prcomp(USArrests,scale=T)$x[,1:2])`.  Then given this plot you can label a subset of countries on the plot by using `text` function in R to add labels at specified positions on the plot.  Please feel free to choose several countries of your preference and discuss the results.  Alternatively, indicate US, UK, China, India, Mexico, Australia, Israel, Italy, Ireland and Sweden and discuss the results.  Where do the countries you have plotted fall in the graph?  Considering what you found out about contributions of different attributes to the first two PCs, what do their positions tell us about their (dis-)similarities in terms of associated health statistics?

```{r}
plot(prcomp(whsAnnBdatNum,scale=T, center=TRUE)$x[,1:2], xlab="PC1 INTINTDS", ylab="PC2 TOTPOP")
countries = c("UnitedStatesofAmerica", "UnitedKingdom", "China", "India", "Mexico", "Australia", "Israel", "Italy", "Ireland")
pr.whs = prcomp(whsAnnBdatNum, scale=T)
text(x=pr.whs$x[countries,1], y=pr.whs$x[countries,2], labels=countries)
```

TOTPOP = total population in thoursands
INTINTDS = reported number of people requiring interventions agains NTDs (neglected tropical diseases)

Most of the countries labeled fall in the lower left hand corner of the plot with 0 or low negative values for INTINTDS and TOTPOP. There are a couple of of exceptions being Mexico, China and India. Mexico and China have negative INTINTDS but positive values for TOTPOP. Conversely, India has a positive value for INTINTDS and negative for TOTPOP. The cluster of nations in the bottom right (US, UK, Australia, Israel, Italy, and Ireland) may be described as nations with controlled populations and low occurances of tropical diseases. They are all geographically further in latitude from the equator which would explain low occurance of tropical diseases, and have stable populations that are neither underpopulated nor overpopulated (UK and Ireland do have TOTPOP of -2, possibly due to lower population levels from smaller nations). 

The dissimilarities in health come from the points outside of the cluster like Mexico, China and India. Mexico has a positive PC2 for total population hinting to issues with overpopulation. Similarly, China may struggle with moderate overpopulation/dense population, but not to the extent of Mexico. China is also dissimlar in variance of PC1 of tropical diseases, with PC1 closer to 0 than the cluster. Depending on geography or advancement in medicine, these may explain a low PC1. India, on the otherhand, has a reasonably stable total population for PC2 but has positive values for tropical diseaseas. It's likely that India has more occurances of tropical disease compared to the cluster due to its equatorial location.

# Problem 2: K-means clustering (20 points)

The goal of this problem is to practice use of K-means clustering and in the process appreciate the variability of the results due to different random starting assignments of observations to clusters and the effect of parameter `nstart` in alleviating it.

## Sub-problem 2a: k-means clusters of different size (5 points)

Using function `kmeans` perform K-means clustering on *explicitly scaled* (e.g. `kmeans(scale(x),2)`) WHS data for 2, 3 and 4 clusters.  Use `cluster` attribute in the output of `kmeans` to indicate cluster membership by color and/or shape of the corresponding symbols in the plot of the first two principal components generated independently on the same (scaled WHS) data.  E.g. `plot(prcomp(xyz)$x[,1:2],col=kmeans(xyz,4)$cluster)` where `xyz` is input data.  Describe the results.  Which countries are clustered together for each of these choices of $K$?

```{r, eval=TRUE}
# k-means cluster
km_2 = kmeans(scale(whsAnnBdatNum), 2)
plot(pca_transformed$x[,1:2], col=(km_2$cluster+1), main="WHS K-means CLusterin K=2")
text(x=pr.whs$x[,1], y=pr.whs$x[,2]+0.5, labels=rownames(whsAnnBdatNum), cex=0.5)

km_3 = kmeans(scale(whsAnnBdatNum), 3)
plot(pca_transformed$x[,1:2], col=(km_3$cluster+1), main="WHS K-means CLusterin K=3")
text(x=pr.whs$x[,1], y=pr.whs$x[,2]+0.5, labels=rownames(whsAnnBdatNum), cex=0.5)

km_4 = kmeans(scale(whsAnnBdatNum), 4)
plot(pca_transformed$x[,1:2], col=(km_4$cluster+1), main="WHS K-means CLusterin K=4")
text(x=pr.whs$x[,1], y=pr.whs$x[,2]+0.5, labels=rownames(whsAnnBdatNum), cex=0.5)

```

For k=2 plot, we see the clusters for countries divided in half based on a PC1 of 0. Cluster 1 consists of a lot of European, North and South American, and Asian coutnries. Cluster 2 consists of mostly African countries The cluster to the left of 0, with negative values represents one cluster corresponding to a negative relation to neglected tropical diseases INTINTDS. And the cluster to the right of 0, with positive values for PC1 corresponds to a positive relation with INTINTDS.

For k=3 plot, we see that the right cluster from k=2 is further subdivided into two more clusters. The left most cluster, values below 0 / negative PC1 values, had a few of its points allocated to the center cluster. The center and right clusters both have postivie PC1 values, but are distinguished in magnitude. The center cluster occupies a range for PC1 close to 0, and the right cluster occupies a range for PC1 greater than 5. The right cluster consists mostly of European, North American, and Asian countries. The center cluster has Mediterranean, North African, and Middle Eastern nations. The right cluster is mostly African countries.

For k=4 plot, subdivides the right cluster further. It is hard to tell exactly which countries are captured with each cluster due to the small font. But mostly European, Asian, and North American countries have negative PC1 and PC2 in the bottom left cluster. The top-left cluster consists of South American and Middle Eastern nations with 0 for PC1 and positive PC2. The center has 0 for PC1 and PC2 and consists of Central Asian countries. And the right cluster has high positive PC1 and close to 0 PC2 and consists of mostly African nations. 

## Sub-problem 2b: variability of k-means clustering and effect of `nstart` parameter (15 points)

By default, k-means clustering uses random set of centers as initial guesses of cluster centers.  Here we will explore variability of k-means cluster membership across several such initial random guesses.  To make such choices of random centers reproducible, we will use function `set.seed` to reset random number generator (RNG) used in R to make those initial guesses to known/controlled initial state.

Using the approach defined above, repeat k-means clustering of *explicitly scaled* WHS data with four (`centers=4`) clusters three times resetting RNG each time with `set.seed` using seeds of 1, 2 and 3 respectively (and default value of `nstart=1`).  Indicate cluster membership in each of these three trials on the plot of the first two principal components using color and/or shape as described above.  Two fields in the output of `kmeans` -- `tot.withinss` and `betweenss` -- characterize within and between clusters sum-of-squares.  Tighter clustering results are those which have smaller ratio of within to between sum-of-squares.  What are the resulting ratios of within to between sum-of-squares for each of these three k-means clustering results (with random seeds of 1, 2 and 3)?

```{r, eval=TRUE}
seeds = c(1,2,3)
nstart = c(1, 100)

tot.withinss = numeric()
betweenss = numeric()

for(start in nstart){
  for(seed in seeds){
    set.seed(seed)
    km = kmeans(scale(whsAnnBdatNum), 4, nstart=start)
    plot(pca_transformed$x[,1:2], col=(km$cluster), main=paste("WHS K-means CLusterin K=4", "Seed=", seed, "NStart=",start, sep=" "))
    tot.withinss = c(tot.withinss, km$tot.withinss)
    betweenss = c(betweenss, km$betweenss)
  }
}
tot.withinss
betweenss
# first three ratios are for nstart=1, the last three ratios are for nstart=100
tot.withinss/betweenss # ratio of within to between for seeds 1, 2, and 3


```

Ratios of withiness to betweeness for sum of squares of the clusters:
1.398089 (nstart=1, seed=1)
1.473971 (nstart=1, seed=2)
1.584992 (nstart=1, seed=3)
1.398089 (nstart=100, seed=1)
1.398089 (nstart=100, seed=2)
1.398089 (nstart=100, seed=3)

Please bear in mind that the actual cluster identity is assigned randomly and does not matter -- i.e. if cluster 1 from the first run of `kmeans` (with random seed of 1) and cluster 4 from the run with the random seed of 2 contain the same observations (country/states in case of WHS dataset), they are *the same* clusters.

Repeat the same procedure (k-means with four clusters for RNG seeds of 1, 2 and 3) now using `nstart=100` as a parameter in the call to `kmeans`.  Represent results graphically as before.  How does cluster membership compare between those three runs now?  What is the ratio of within to between sum-of-squares in each of these three cases?  What is the impact of using higher than 1 (default) value of `nstart`?  What is the ISLR recommendation on this offered in Ch. 10.5.1?

Cluster membership does not change or does not seem to change with an nstart=100. According to the kmeans() function, nstart conducts multiple initial configurations that chooses the best centroids from the algrith. It seems that more times this configuration is run (nstart is higher), the more consistent the clsutering becomes as the algorithm/computation agrees on the optimal clustering pattern. The ISLR book recommendeds a large nstart of 20 or 50, likely to provide consistant clustering results. If we look at the  ratio of the withiness to betweeness for sum of squares of each cluster, the ratio increases with each seed for nstart=1. This goes against the goal of k-means to minimize the withiness. However, the ratio is constant for all three seeds when nstart=100. We find that setting a higher nstart is more effective at controlling the consistancy of the clustering compared to setting the seed. 

One way to achieve everything this sub-problem calls for is to loop over `nstart` values of 1 and 100, for each value of `nstart`, loop over RNG seeds of 1, 2 and 3, for each value of RNG seed, reset RNG, call `kmeans` and plot results for each combination of `nstart` and RNG seed value.


# Problem 3: Hierarchical clustering (15 points)

## Sub-problem 3a: hierachical clustering by different linkages (10 points)

Cluster country states in (scaled) world health statistics data using default (Euclidean) distance and "complete", "average", "single" and "ward" linkages in the call to `hclust`.  Plot each clustering hierarchy, describe the differences.  For comparison, plot results of clustering *untransformed* WHS data using default parameters (Euclidean distance, "complete" linkage) -- discuss the impact of the scaling on the outcome of hierarchical clustering.

```{r}
nonscale_dat = whsAnnBdatNum
scale_dat = scale(whsAnnBdatNum)

# hierarchical clustering for scaled data
data_dist = dist(scale_dat, method="euclidean")
hc_complete_sca = hclust(data_dist, method="complete")
hc_average_sca = hclust(data_dist, method="average")
hc_single_sca = hclust(data_dist, method="single")
hc_ward_sca = hclust(data_dist, method="ward.D")

# plot
plot(hc_complete_sca, main="Complete Linkage Scaled", xlab=NULL, sub="", cex=0.5)
plot(hc_average_sca, main="Average Linkage Scaled", xlab=NULL, sub="", cex=0.5)
plot(hc_single_sca, main="Single Linkage Scaled", xlab=NULL, sub="", cex=0.5)
plot(hc_ward_sca, main="Ward Linkage Scaled", xlab=NULL, sub="", cex=0.5)

# hierarchical clustering for unscaled data
dist = dist(nonscale_dat, method="euclidean")
hc.complete = hclust(dist, method="complete")
hc.average = hclust(dist, method="average")
hc.single = hclust(dist, method="single")
hc.ward = hclust(dist, method="ward.D")

# plot
plot(hc.complete, main="Complete Linkage Nonscaled", xlab=NULL, sub="", cex=0.5)
plot(hc.average, main="Average Linkage Nonscaled", xlab=NULL, sub="", cex=0.5)
plot(hc.single, main="Single Linkage Nonscaled", xlab=NULL, sub="", cex=0.5)
plot(hc.ward, main="Ward Linkage Nonscaled", xlab=NULL, sub="", cex=0.5)

```

When comparing each scaled method with its unscaled counterpart, we see that the effect of scaling helps balance the cluster sizes. The last four plots for unscaled data place almost all the observations into a single cluster or two as k increases, with a separate outlier cluster for India. On the otherhand, scaling tries to create balanced clusters as k increases, based on each method type. And we see typical behavior in the scaled data with Single linkage linking one large cluster together and having a much smaller cluster counterpart. Complete provides several distinct clusters that are balanced in size for higher k's. And average gives an intermediate structure of the two. Ward also has a balance that splits into multiple clusters after the intial split of two clusters.

## Sub-problem 3b: compare k-means and hierarchical clustering (5 points)

Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and "complete" linkage.  Use function `table` to compare membership of these clusters to those produced by k-means clustering with four clusters in the Problem 2(b) when using `nstart=100` (and any of the RNG seeds) above.  Discuss the results.

```{r, eval=TRUE}
hc_4 = cutree(hc_complete_sca, 4)
# k-means k = 4, nstart=100
set.seed(4)
km_4 = kmeans(scale(whsAnnBdatNum), 4, nstart=100)
km4_cluster = km_4$cluster

table(hc_4)
table(km4_cluster)
```

The membership counts for each of the four clusters of data are listed in the tables above. The first table is the membership for Hierarchical Clustering with four clusters, k=4. The second table is the membership coutns for k-Means Clustering with four clusters, k=4 and nstart=100. First it is interesting to note the spread on the Hierarchical method. Almost all the points are concentrated within the first two clusters, 55 and 137, and only one point each for cluster three and four. We knew previously from the graph that one of these solo point clusters contains India, which has been classified by the clustering algorithm to have distinct features that make the nation an outlier. We can also make out that another solo cluster belongs to the SyrianArabRepublic making it the other outlier. 

With the k-Means Clustering method, we see the membership more evenly distributed among the clusters compared to Hierarchical Clustering. Cluster three had the largest membership count but does not consist of a majority of the data. Based on these counts, we can see that Hierarchical Clustering method is susceptible to the influence of outliers when constructing the clusters. However, we can see in the graph for Complete clustering that as the k gets larger, the membership sizes even out. And it is important to pay attention to the outlier clusters and what sort of information we can extract about single members such as India or SyrianArabRepublic. Such as what defining attributes warrant their own cluster. And we also see that using k-Means can be beneficial for looking at large group similarities. These may be more general attributes that separate a sizeable group of nations from another, but are blanket characteristics applicable to a certain class of nations.  

# Appendix: pre-processing of WHS data

For your reference only -- the file it has generated is already available at our course website

```{r WHSpreproc,eval=FALSE}
whsAnnBdat <- read.table("../data/whs2016_AnnexB-data.txt",sep="\t",header=T,as.is=T,quote="")
dim(whsAnnBdat)
whsAnnBdat <- apply(whsAnnBdat,2,function(x)gsub(">","",gsub("<","",gsub(" ","",x))))
whsAnnBdat <- apply(whsAnnBdat,2,function(x){x[x==rawToChar(as.raw(150))]<-"";x})
rownames(whsAnnBdat) <- whsAnnBdat[,1]
whsAnnBdat <- whsAnnBdat[,-1]
whsAnnBdatNum <- apply(whsAnnBdat,2,as.numeric)
whsAnnBdatNum <- apply(whsAnnBdatNum,2,function(x){x[is.na(x)] <- mean(x,na.rm = TRUE);x})
rownames(whsAnnBdatNum) <- rownames(whsAnnBdat)
write.table(whsAnnBdatNum,"../data/whs2016_AnnexB-data-wo-NAs.txt",quote=F,sep="\t")
```
