---
title: "CSCI E-63C Week 11 Problem Set"
author: "Erik Lee"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(ISLR)
library(e1071)
library(randomForest)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```


# Preface

This week problem set will explore behavior of support vector classifiers and SVMs (following the distinction made in ISLR) on banknote authentication dataset from UCI ML archive.  We worked with it on multiple occasions before (most recently two weeks ago evaluating performance of logistic regression, discriminant analysis and KNN on it):

```{r dbaExample}
dbaDat <- read.table("data_banknote_authentication.txt",sep=",")
colnames(dbaDat) <- c("var","skew","curt","entr","auth")
dbaDat$auth <- factor(dbaDat$auth)
dim(dbaDat)
summary(dbaDat)
head(dbaDat)
pairs(dbaDat[,1:4],col=as.numeric(dbaDat$auth))
```

Here we will use SVM implementation available in library `e1071` to fit classifiers with linear and radial (polynomial for extra points) kernels and compare their relative performance as well as to that of random forest and KNN.

# Problem 1 (20 points): support vector classifier (i.e. using linear kernel) 

Use `svm` from library `e1071` with `kernel="linear"` to fit classifier (e.g. ISLR Ch.9.6.1) to the entire banknote authentication dataset setting parameter `cost` to 0.001, 1, 1000 and 1 mln.  Describe how this change in parameter `cost` affects model fitting process (hint: the difficulty of the underlying optimization problem increases with cost -- can you explain what drives it?) and its outcome (how does the number of support vectors change with `cost`?) and what are the implications of that.  Explain why change in `cost` value impacts number of support vectors found. (Hint: there is an answer in ISLR.)  Use `tune` function from library `e1071` (see ISLR Ch.9.6.1 for details and examples of usage) to determine approximate value of cost (in the range between 0.1 and 100 -- the suggested range spanning ordes of magnitude should hint that the density of the grid should be approximately logarithmic -- e.g. 1, 3, 10, ... or 1, 2, 5, 10, ... etc.) that yields the lowest error in cross-validation employed by `tune`.  Setup a resampling procedure repeatedly splitting entire dataset into training and test, using training data to `tune` cost value and test dataset to estimate classification error. Report and discuss distributions of test errors from this procedure and selected values of `cost`.

```{r}
# Fit SVM for Cost 0.001, 1, 1000, 1000000
svm.fit.cost.0.001 = svm(dbaDat$auth~., data=dbaDat, kernel="linear", cost=0.001, scale=FALSE)
summary(svm.fit.cost.0.001)

svm.fit.cost.1 = svm(dbaDat$auth~., data=dbaDat, kernel="linear", cost=1, scale=FALSE)
summary(svm.fit.cost.1)


svm.fit.cost.1000 = svm(dbaDat$auth~., data=dbaDat, kernel="linear", cost=1000, scale=FALSE)
summary(svm.fit.cost.1000)

svm.fit.cost.1mil = svm(dbaDat$auth~., data=dbaDat, kernel="linear", cost=1000000, scale=FALSE)
summary(svm.fit.cost.1mil)

costs = c(0.001, 1, 1000, 1000000)
numSV = c(sum(svm.fit.cost.0.001$nSV), sum(svm.fit.cost.1$nSV), sum(svm.fit.cost.1000$nSV), sum(svm.fit.cost.1mil$nSV))

plot(log(costs), numSV, main="log(cost) vs. # of Support Vectors")

```

Now, we do see in the results above the number of support vectors for each respective Cost of 0.001, 1, 1000, 1000000 to be 409, 43, 20, 9 SVs, respectively. We see a trend in the plot of the number of Support Vectors versus log-cost that as the cost increases, the number of support vectors decreases (given that the regression equation, data, kernel, and svm function are consistent). The Cost parameter is an optimization that determines how severe the penalty is for misclassifying training observations.

As the cost parameter increases, the number of support vectors decreases indicating a smaller margin and higher penalty for missclassifying observations. Conversely, as the cost parameter decreases, the number of support vectors increase indicating a larger margin and lower penalty for misclassifying observations. We saw there were issues with converging the data at costs 1,000 and 1,000,000 and this is due to the issue with low numbers of support vectors and using a linear decision boundary. At high cost levels, the number of support vectors is low, and these support vectors help define the linear decision boundary. With a low number of support vectors to define the boundary, the algorithm has difficulty creating an accurate boundary to classify the training observations. 

```{r, eval=TRUE}
# Set the seed for consistent results
set.seed(3)

# Use tune function to find the best Cost model
tune.out = tune(svm, dbaDat$auth~dbaDat$var+dbaDat$skew+dbaDat$curt+dbaDat$entr, data=dbaDat, kernel="linear",
                ranges=list(cost=c(0.1, 1, 10, 100)))
summary(tune.out)
```

Based on the tune function, the best model, with lowest training MSE, has a Cost of 10. This model has a training MSE of 0.01078717.

```{r,eval=TRUE}
# CrossValidation
set.seed(3)

dfTemp = NULL

nTries = 10

for(i in 1:nTries){
  
  train = sample(dim(dbaDat)[1], size=dim(dbaDat)[1]/2) # 1 = training, 2 = test
  train.data = dbaDat[train, ]
  test.data = dbaDat[-train, ]
  
  tune.out = tune(svm, train.data$auth~train.data$var+train.data$skew+train.data$curt+train.data$entr, data=train.data, kernel="linear",
                ranges=list(cost=c(0.1, 1, 10, 100)))
  svm.best.fit = tune.out$best.model
  ypredict = predict(svm.best.fit, newdata=test.data)
  
  tab = table(predict=ypredict, truth=dbaDat$auth[-train])
  testMSE = 1-(tab[1,1] + tab[2,2])/sum(tab)
  
  dfTemp = rbind(dfTemp, data.frame(Iter=i, Cost=tune.out$best.parameters, testMSE=testMSE))
    
}

dfTemp
mean(dfTemp$testMSE[dfTemp$cost == 10]) # mean testMSE for Cost = 10
mean(dfTemp$testMSE[dfTemp$cost == 100]) # mean testMSE for Cost = 100
mean(dfTemp$testMSE) # average test error for 10 iterations
```

Based on the Cross-Validation of the data set above, we used the tune() function to find the best Cost value for the training dataset and tested the SVM model on a separate test dataset from the same data. We repeated this cross-validation ten times to produce varying cost values and test MSEs. Originally, the range of Cost values was limited to 0.1, 1, 10, and 100. From the dataframe above, we see that three of the ten cross-validations selected a Cost of 100, and the rest selected a Cost of 10. This is consistent with our first call of tune() on the entire dataset, which selected Cost = 10 as the best model. For the 10 iterations, we have an average test error of 0.4906706.  Nonetheless, we have solid evidence to support using Cost of 10 for our SVM model fit for this particular Bank Note dataset.

# Problem 2 (10 points): comparison to random forest

Fit random forest classifier on the entire banknote authentication dataset with default parameters.  Calculate resulting misclassification error as reported by the confusion matrix in random forest output.  Explain why error reported in random forest confusion matrix represents estimated test (as opposed to train) error of the procedure.  Compare resulting test error to that for support vector classifier obtained above and discuss results of such comparison.

```{r, eval=TRUE}
rf.bank = randomForest(dbaDat$auth~., data=dbaDat)
rf.bank$confusion
sum(rf.bank$confusion[, 3])
```

After fitting a random forest classifier to the entire banknote dataset and obtaining the confusion matrix, the calculated missclassification error is 0.01410438. The Random Forest function internally calculates the Out-Of-Bag (OOB) MSE, which is the test/misclassification error. OOB MSE is calculated by predicting the class for observations on trees that omitted them in construction (about a third of the trees). 

Comparing random forrest test error to the 10-fold CV from the tune() function, which has a test error of 0.01078717, it seems that Cost 10 SVM performs slightly better at classifying the entire dataset with lower error. However, it is important note that each of these test errors, for Random Forests and 10-fold CV for Cost 10 SVM, were generated by testing on the banknote dataset. We have yet to see how each classifier model performs on a new test dataset. 

We can also compare these results to the cross-validation above. We found from cross-validation has a avg. test error of 0.4906706. This is much higher than the test error of random forest at 0.01410438. But it is important to note that the cross-validation used random sets of half the data to train on and the other half of the data to test on. Essentially, half the information was withheld from the SVM fit, and the predictions and test error suffer for cross-validation. 

# Problem 3 (10 points): Comparison to cross-validation tuned KNN predictor

Use convenience wrapper `tune.knn` provided by the library `e1071` on the entire dataset to determine optimal value for the number of the nearest neighbors 'k' to be used in KNN classifier.  Consider our observations from week 9 problem set when choosing range of values of `k` to be evaluated by `tune.knn`.  Setup resampling procedure similar to that used above for support vector classifier that will repeatedly: a) split banknote authentication dataset into training and test, b) use `tune.knn` on training data to determine optimal `k`, and c) use `k` estimated by `tune.knn` to make KNN classifications on test data.  Report and discuss distributions of test errors from this procedure and selected values of `k`, compare them to those obtained for random forest and support vector classifier above.

```{r, eval=TRUE}
set.seed(3)

# tune.knn on whole dataset
x = dbaDat[,-5]
y = dbaDat[, 5]
knn.out = tune.knn(x=x, y=y, k=c(1,2,5,11,21,51,101))
summary(knn.out)
```

Based on the run of tune.knn on the entire dataset, the best value for K is 11. It is odd that the training error and dispersion are both 0. This may be an error in the function or call of the function.

```{r, eval=TRUE}
# Cross Validation for KNN
set.seed(3)

dfTemp2 = NULL
nTries = 10

for(i in 1:nTries){
  train = sample(dim(dbaDat)[1], size=dim(dbaDat)[1]/2)
  train.data = dbaDat[train, ]
  test.data = dbaDat[-train, ]
  
  knn.out = tune.knn(x=train.data[,-5], y=train.data[,5], k=c(1,2,5,11,21,51,101))
  knn.best.fit = knn.out$best.model
  
  # prediction
  ypred = knn(train=train.data, test=test.data, cl=train.data[,5], k=knn.best.fit$k)
  tab = table(ypred, test.data[, 5])
  
  testMSE = 1-(tab[1,1] + tab[2,2])/sum(tab)
  dfTemp2 = rbind(dfTemp2, data.frame(Iter=i, K=knn.out$best.parameters, testMSE=testMSE))
}

dfTemp2
mean(dfTemp2$testMSE[dfTemp2$k==1])
mean(dfTemp2$testMSE[dfTemp2$k==2])
mean(dfTemp2$testMSE) # average test error for 10 iters

```

After running cross-validation ten times using the tune.knn() function, we see the resulting K values and test error for each runs "Best Fit" model. First, we see that K = 1 is the most popular K value found in 9 out of 10 cross-validations, and K = 2 was chosen in 1 out of 10 cross-validations. We also see that K = 1 has an average test error of 0.0009718173, while the one run of K = 2 has a test error of 0. Although K = 2 had a lower test error on a solo run, K = 1 was chosen overwhelmingly in each trial.

Now it is important to note that of the values of K availible to tune.knn (1,2,5,11,21,51,101), the function chose low values of K. This presents an issue of overfitting to the training data as low K values have low bias but very high variance. In other words, using the next nearest neighbor or two causes the model to fit extremely well to the training data, the bank note data, but may lead to poor performance on completely new test data.

Having said that, we can see that KNN does have lower test error comapred to linear SVM tune (0.01078717 for 10-fold CV). Comparatively, tune.knn with 10-fold CV on the entire bank note dataset chose K = 11 for best performance with a error of 0. Comparing the cross-validations test error, linear SVM had a avg. test error of 0.4906706 and KNN beats this, and Random Forrest (0.01410438),  with 0.0009718173. While KNN has great performance on this dataset, we have to keep in mind the overfitting that comes with KNN, and note that we need to test these results on new test data to feel more confident. 


# Problem 4 (20 points): SVM with radial kernel

## Sub-problem 4a (10 points): impact of $gamma$ on classification surface

*Plot* SVM model fit to the banknote authentication dataset using (for the ease of plotting) *only variance and skewness* as predictors variables, `kernel="radial"`, `cost=1` and `gamma=1` (see ISLR Ch.9.6.2 for an example of that done with a simulated dataset).  You should be able to see in the resulting plot the magenta-cyan classification boundary as computed by this model.  Produce the same kinds of plots using 0.01 and 100 as values of `gamma` also.  Compare classification boundaries between these three plots and describe how they are impacted by the change in the value of `gamma`.  Can you trace it back to the role of `gamma` in the equation introducing it with the radial kernel in ISLR?

```{r, eval=TRUE}
# radial svm, gamma=1
new.dbaDat = dbaDat[, c(5,1,2)]
svm.fit.gamma.1 = svm(auth~var+skew, data=new.dbaDat, kernel="radial", cost=1, gamma=1)
plot(svm.fit.gamma.1, data=new.dbaDat)

# gamma = 0.01
svm.fit.gamma.0.01 = svm(auth~var+skew, data=new.dbaDat, kernel="radial", cost=1, gamma=0.01)
plot(svm.fit.gamma.0.01, data=new.dbaDat)

# gamma = 100
svm.fit.gamma.100 = svm(auth~var+skew, data=new.dbaDat, kernel="radial", cost=1, gamma=100)
plot(svm.fit.gamma.100, data=new.dbaDat)
```

Above we see three plots in order for Gamma of 1, 0.01, and 100 respectively. The plots show vaiance against skewness, the bank note dataset observations represented by X's and O's, classifcation of 0 in cyan and 1 in magenta, and the resulting radial decision boundary from SVM. The X marked observations are the Support Vectors that define the radial decision boundary.

For Gamma of 1, we see a decision boundary that curves outward from the magenenta area with a very noticible hump at the center, where lots of Red points (classified as 1) are clustered. Almost all the red points fall in magenta and black points fall in cyan as they should, and the decision boundary seems to follow the separation of data well. 

For Gamma of 0.01, we see an almost linear decision boundary. The cyan and magenta area are well defined. However, the decision boundary fails to pick up the red cluster hump at the center of the graph am misclassifies that portion of observations. This model looks less closely tied to the training data and more generalizable. Lastly, the gamma of 100 has a very tight boundary around all of the red points. The boundary follows closely all values of 1 in the bank note training data, and lazily classfies everything outside the proximity of magenta as cyan (points of value 0). This model looks heavilyt tied to the training data and not generalizable to other test data.

A small gamma has high variance giving it more influence to decide the class of a given point, even if their distance between is large. A large gamma has high bias and low variance where support vecotrs do not have wide influence and act on local points. Based on the graphs, we see such case with low Gamma of 0.01 having a very generalized, almost linear decision boundary where support vectors on far from the boundary have influence. Conversely, for Gamma of 100, we had a very tight decision boundary because the redpoints very close to the boundary define the magenta area and decison boundary, leading to tight fit and high variance. 


## Sub-problem 4b (10 points): test error for SVM with radial kernel

Similar to how it was done above for support vector classifier (and KNN), set up a resampling process that will repeatedly: a) split the entire dataset (using all attributes as predictors) into training and test datasets, b) use `tune` function to determine optimal values of `cost` and `gamma` and c) calculate test error using these values of `cost` and `gamma`.  You can start with `cost=c(1,2,5,10,20)` and `gamma=c(0.01,0.02,0.05,0.1,0.2)` as starting ranges to evaluate by `tune`, but please feel free to experiment with different sets of values and discuss the results of it and how you would go about selecting those ranges starting from scratch.  Present resulting test error graphically, compare it to that of support vector classifier (with linear kernel), random forest and KNN classifiers obtained above and discuss results of these comparisons. 

```{r, eval=TRUE}
# Cross-Validation for Radial SVM
set.seed(3)

dfTemp3 = NULL
nTries = 50 # up the iterations to 50 because a lot of the testMSE is at 0

for(i in 1:nTries){
  train = sample(dim(dbaDat)[1], size=dim(dbaDat)[1]/2)
  train.data = dbaDat[train, ]
  test.data = dbaDat[-train, ]
  
  svm.radial.out = tune(svm, auth~., data=train.data, kernel="radial", 
                        ranges=list(cost=c(1,2,5,10,20), gamma=c(0.01,0.02,0.05,0.1,0.2)))
  svm.radial.best.fit = svm.radial.out$best.model
  
  ypredict = predict(svm.radial.best.fit, newdata=test.data)
  tab = table(predict=ypredict, truth=dbaDat$auth[-train])
  testMSE = 1-((tab[1,1] + tab[2,2])/sum(tab))
  
  dfTemp3 = rbind(dfTemp3, data.frame(Iter=i, Cost=svm.radial.out$best.parameters$cost,
                                      Gamma=svm.radial.out$best.parameters$gamma, testMSE=testMSE))
}

dfTemp3

```

When selecting parameters such as Cost and Gamma for SVM, it is important to weight the bias-variance tradeoff and consider performance on test data. As mentioned before, Cost for SVM determines how much we want to penalize misclassified observations. High cost means little or no allowance of margin or misclassified observations, and low cost allows for more misclassifications. Gamma determines which points will have influence over the decision boundary. Low Gamma values allow for support vectors far away to influence boundary. High Gamma values allow support vectors to only act locally on points causing a tigh fit to the data.

To choose these parameters, we can use tune() and cross-validation to select values. In our previous answers, we found that Cost between 0.01 and 10 are decent values that will converge and provide decent test errors. We also found that low Gamma values between 0.01 and 1 can provide a flexible model with decent classification boundary. Then we select values between these two ranges to test best performance. 

```{r, eval=TRUE}
set.seed(3)

# plots of dfTemp3 containing Iter, Cost, Gamma, and TestMSE
ggplot(dfTemp3, aes(x=factor(Cost), y=testMSE, colour=Cost)) + geom_boxplot()
ggplot(dfTemp3, aes(x=factor(Gamma), y=testMSE, colour=Gamma)) + geom_boxplot()

# histogram of which was chose most often for best.model
hist(dfTemp3$Cost)
hist(dfTemp3$Gamma)

# mean test error for Cost = 20 and Gamma = 0.05 (best parameters)
mean(dfTemp3$testMSE[dfTemp3$Cost==20]) # test error for all Cost = 20
mean(dfTemp3$testMSE[dfTemp3$Gamma==0.05]) # test error for all Gamma = 0.05
mean(dfTemp3$testMSE[dfTemp3$Cost==20 & dfTemp3$Gamma==0.05])

# average test error for all 50 iterations
mean(dfTemp3$testMSE)

# 10-fold CV with tune on Radial SVM
svm.radial.out = tune(svm, auth~., data=dbaDat, kernel="radial", 
                        ranges=list(cost=c(1,2,5,10,20), gamma=c(0.01,0.02,0.05,0.1,0.2)))
summary(svm.radial.out)

```

Above, we have two boxplots for the performance on differnt Costs and different Gammas selected by tune(). From the first set of the code, we performed 50 iterations of the cross-validation, splitting the data equally into training and test datasets. We used a selection of Cost = 1,2,5,10,20 and Gamma = 0.01,0.02,0.05,0.1,0.2, and we ask each iteration to fit the best radial model and calculate the respective test errors. The histrograms show the frequencies of which Cost and which Gamma were selected the most in the 50 iterations.

We see in the first boxplot that Cost 5, 10, and 20 were selected. Cost 20 seems to have the highest variance in test error and two outliers. Cost 5 and 10 have equal performance with median test error at 0 and one outlier. But in the histogram, we see that Cost 20 was selected in more than half the models so the results seem to favor Cost 20.

We see in the second boxplot graph that Gamma 0.02, 0.05, 0.1, and 0.2 were selected by iterations of tune(). Gamma 0.1 and 0.2 tie for the best performance followed by 0.05, and then 0.02 with the highest test error. In the histogram of Gamma frequencies, the iterations favored Gamma 0.05 with over half the models, so our cross-validations support Gamma of 0.05. 

For the 10-fold CV runs (train with entire dataset), radial SVM has a error of 0, compared to linear SVM at 0.01078717, Random Forest (OOB Error) at 0.01410438, and KNN (tune.knn) at 0. We see that radial SVM outperforms linear SVM and Random Forest, and ties KNN when the whole dataset is fit for each type of classification.

When looking at test error, we have an average test error of 0.0009329446 for all 50 iterations. Before, we saw linear SVM cross-validation test error at 0.4906706, Random Forest misclassification (test) error at 0.01410438, and KNN cross-validation error at 0.0009718173. Again, radial SVM outperforms linear SVM and Random Forest, and ties KNN for CV test. 

We see that radial SVM and KNN have advantages for classifying the bank note data. This makes sense since the observations have a curved decision boundary with the hump, and classifiers that can handle the curved boundary perform better. And we should also note that radial SVM and KNN have similar concept by classifying observations based on nearest neighboring observations (or support vectors in the case of radial SVM). If we were to test our models on a test dataset, we may want to move forward with either Radial SVM or KNN.

# Extra 10 points problem: SVM with polynomial kernel

Repeat what was done above (plots of decision boundaries for various interesting values of tuning parameters and test error for their best values estimated from training data) using `kernel="polynomial"`.   Determine ranges of `coef0`, `degree`, `cost` and `gamma` to be evaluated by `tune`.  Present and discuss resulting test error and how it compares to linear and radial kernels and those of random forest and KNN.

```{r, eval=TRUE}
set.seed(3)

# tune function to find optimal parameter values; 10-fold CV over entire bank note dataset
svm.poly.best.fit = tune(svm, auth~., data=dbaDat, kernel="polynomial", 
                         ranges=list(cost=c(1,5,10,20), gamma=c(0.01, 0.05, 0.2), coef=c(0,1), 
                                     degree=c(1,2,3)))
summary(svm.poly.best.fit)
# plot the decision boundary for different coef0, degree, cost, and gamma

```

For the run of tune on polynomial SVM with the entire dataset, the optimal parameters are Cost=10, Gamma=0.05, Coef=1, and Degree=2. Now, we will plot these parameters and look at the decision boundary. But we will alter Coef to 0 or 1 and Degree 1, 2, and 3.

```{r, eval=TRUE}
# polynomial svm; all cost=10 and gamma=0.05
new.dbaDat = dbaDat[, c(5,1,2)]

# coef=0, degree=2
svm.fit.1 = svm(auth~var+skew, data=new.dbaDat, kernel="polynomial", cost=10, gamma=0.05, coef=0, degree=2)
plot(svm.fit.1, data=new.dbaDat)

# coef=1, degree=1
svm.fit.2 = svm(auth~var+skew, data=new.dbaDat, kernel="polynomial", cost=10, gamma=0.05, coef=1, degree=1)
plot(svm.fit.2, data=new.dbaDat)

# coef=1, degree=2
svm.fit.3 = svm(auth~var+skew, data=new.dbaDat, kernel="polynomial", cost=10, gamma=0.05, coef=1, degree=2)
plot(svm.fit.3, data=new.dbaDat)

# cpef=1, degree=3
svm.fit.4 = svm(auth~var+skew, data=new.dbaDat, kernel="polynomial", cost=10, gamma=0.05, coef=1, degree=3)
plot(svm.fit.4, data=new.dbaDat)

```

Based on a cursory look of the graph, we have each graph varying in coefficient and degree. The first graph has a coefficient=0 and degree=1. It provides the poorest classification with many misclassified points for 1 (red) on the magenta, and the boundaries are not clearly separated with too many support vectors. The second graph with coef=1 and degree=1 acts almost like a linear boundary. It does a decent job separating classes and has flexibility, but it misses the center cluster hump of red points.

The second graph with coef=1 and degree=2 and third graph with coef=1 and degree 3 are very similar. They both have a curved boundary characteristic of the polynomial and wraps the magenta region and boundary around the points for 1 (red points). The last graph with degree=3 has a tighter fit to the data points having the boundary closer to the red support vectors.

```{r}
# cross-validation for polynomial SVM
set.seed(3)

dfTemp4 = NULL
nTries = 20

for(i in 1:nTries){
  train = sample(dim(dbaDat)[1], size=dim(dbaDat)[1]/2)
  train.data = dbaDat[train, ]
  test.data = dbaDat[-train, ]
  
  svm.poly.out = tune(svm, auth~., data=train.data, kernel="polynomial", cost=10, gamma=0.05,
                      ranges=list(coef=c(0,1), degree=c(1,2,3)))
  svm.poly.best.fit = svm.poly.out$best.model
  
  ypredict = predict(svm.poly.best.fit, newdata=test.data)
  tab = table(predict=ypredict, truth=dbaDat$auth[-train])
  testMSE = 1-((tab[1,1] + tab[2,2])/sum(tab))
  
  dfTemp4 = rbind(dfTemp4, data.frame(Iter=i, Cost=10, Gamma=0.05, Coef=svm.poly.out$best.parameters$coef, Degree=svm.poly.out$best.parameters$degree, testMSE=testMSE))
}

```

```{r}
set.seed(3)

# plots of dfTemp3 containing Iter, Cost, Gamma, and TestMSE
ggplot(dfTemp4, aes(x=factor(Coef), y=testMSE, colour=Coef)) + geom_boxplot()
ggplot(dfTemp4, aes(x=factor(Degree), y=testMSE, colour=Degree)) + geom_boxplot()

# histogram of which was chose most often for best.model
hist(dfTemp4$Degree)

# average test error for 20 iterations
mean(dfTemp3$testMSE)
```

We see from the results that, when running the polynomial SVM on the bank note dataset, Cost=10, Gamma=0.05, Coef=1, and Degree=3 are the optimal parameters. We see in the boxplot of Coef that only Coef=1 was chosen between options of 0 or 1. And we see that in the majority of runs for cross-validation Degree=3 is favored over Degree=2.

In the training of the entire dataset with tune, the best model had Cost=10, Gamma=0.05, Coef=1, Degree=2 and a error of 0. This is similar to the run with radial SVM (error=0) and tune.knn (error=0), and beats linear SVM (error=0.01078717) and Random Forest (error=0.01410438)

In terms of cross-validation, polynomial SVM has an average test error of 0.0009329446 over 20 iterations. This is a competative test error with radial SVM (test error = 0.0009329446) and KNN (test error = 0.0009718173) cross-validations. Polynomial SVM beats linear SVM (test error = 0.4906706) and misclassification of Random Forest (OOB error = 0.01410438)

Based on the results and comparisons with different classification methods, polynomial also seems like a viable choice for classifying Bank Note data. Along with radial SVM and KNN, polnomial SVM may have decent performance on a new dataset. Addionally, the ability to tune Coefficients and Degree has an added control advantage over the choices for radial SVM and KNN. 

